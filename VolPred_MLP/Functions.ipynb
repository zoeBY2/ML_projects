{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO0OYF/Bv9R862h9rOtNfB+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2z8VytOpxKXP"},"outputs":[],"source":["import os, gc\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from hyperopt import hp, fmin, tpe, Trials\n","from hyperopt.pyll.base import scope\n","from sklearn.metrics import roc_auc_score, roc_curve\n","from tqdm.notebook import tqdm\n","from joblib import dump, load\n","\n","import tensorflow as tf\n","tf.random.set_seed(42)\n","import tensorflow.keras.backend as K\n","import tensorflow.keras.layers as layers\n","from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n","\n","from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n","from sklearn.utils.validation import _deprecate_positional_args\n","from sklearn.model_selection import GroupKFold, KFold\n","from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n","\n"]},{"cell_type":"code","source":["def weighted_average(a):\n","    w = []\n","    n = len(a)\n","    for j in range(1, n + 1):\n","        j = 2 if j == 1 else j\n","        w.append(1 / (2**(n + 1 - j)))\n","    return np.average(a, weights = w)"],"metadata":{"id":"AFPEMGLmxNps"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GroupTimeSeriesSplit(_BaseKFold):\n","    def __init__(self,\n","                 n_splits=5,\n","                 *,\n","                 max_train_size=None\n","                 ):\n","        super().__init__(n_splits, shuffle=False, random_state=None)\n","        self.max_train_size = max_train_size\n","\n","    def split(self, X, y=None, groups=None):\n","        if groups is None:\n","            raise ValueError(\n","                \"The 'groups' parameter should not be None\")\n","        X, y, groups = indexable(X, y, groups)\n","        n_samples = _num_samples(X)\n","        n_splits = self.n_splits\n","        n_folds = n_splits + 1\n","        group_dict = {}\n","        u, ind = np.unique(groups, return_index=True)\n","        unique_groups = u[np.argsort(ind)]\n","        n_samples = _num_samples(X)\n","        n_groups = _num_samples(unique_groups)\n","        for idx in np.arange(n_samples):\n","            if (groups[idx] in group_dict):\n","                group_dict[groups[idx]].append(idx)\n","            else:\n","                group_dict[groups[idx]] = [idx]\n","        if n_folds > n_groups:\n","            raise ValueError(\n","                (\"Cannot have number of folds={0} greater than\"\n","                 \" the number of groups={1}\").format(n_folds,\n","                                                     n_groups))\n","        group_test_size = n_groups // n_folds\n","        group_test_starts = range(n_groups - n_splits * group_test_size,\n","                                  n_groups, group_test_size)\n","        for group_test_start in group_test_starts:\n","            train_array = []\n","            test_array = []\n","            for train_group_idx in unique_groups[:group_test_start]:\n","                train_array_tmp = group_dict[train_group_idx]\n","                train_array = np.sort(np.unique(\n","                                      np.concatenate((train_array,\n","                                                      train_array_tmp)),\n","                                      axis=None), axis=None)\n","            train_end = train_array.size\n","            if self.max_train_size and self.max_train_size < train_end:\n","                train_array = train_array[train_end -\n","                                          self.max_train_size:train_end]\n","            for test_group_idx in unique_groups[group_test_start:\n","                                                group_test_start +\n","                                                group_test_size]:\n","                test_array_tmp = group_dict[test_group_idx]\n","                test_array = np.sort(np.unique(\n","                                              np.concatenate((test_array,\n","                                                              test_array_tmp)),\n","                                     axis=None), axis=None)\n","            yield [int(i) for i in train_array], [int(i) for i in test_array]\n","\n","class PurgedGroupTimeSeriesSplit(_BaseKFold):\n","    @_deprecate_positional_args\n","    def __init__(self,\n","                 n_splits=5,\n","                 *,\n","                 max_train_group_size=np.inf,\n","                 max_test_group_size=np.inf,\n","                 group_gap=None,\n","                 verbose=False\n","                 ):\n","        super().__init__(n_splits, shuffle=False, random_state=None)\n","        self.max_train_group_size = max_train_group_size\n","        self.group_gap = group_gap\n","        self.max_test_group_size = max_test_group_size\n","        self.verbose = verbose\n","\n","    def split(self, X, y=None, groups=None):\n","        if groups is None:\n","            raise ValueError(\n","                \"The 'groups' parameter should not be None\")\n","        X, y, groups = indexable(X, y, groups)\n","        n_samples = _num_samples(X)\n","        n_splits = self.n_splits\n","        group_gap = self.group_gap\n","        max_test_group_size = self.max_test_group_size\n","        max_train_group_size = self.max_train_group_size\n","        n_folds = n_splits + 1\n","        group_dict = {}\n","        u, ind = np.unique(groups, return_index=True)\n","        unique_groups = u[np.argsort(ind)]\n","        n_samples = _num_samples(X)\n","        n_groups = _num_samples(unique_groups)\n","        for idx in np.arange(n_samples):\n","            if (groups[idx] in group_dict):\n","                group_dict[groups[idx]].append(idx)\n","            else:\n","                group_dict[groups[idx]] = [idx]\n","        if n_folds > n_groups:\n","            raise ValueError(\n","                (\"Cannot have number of folds={0} greater than\"\n","                 \" the number of groups={1}\").format(n_folds,\n","                                                     n_groups))\n","\n","        group_test_size = min(n_groups // n_folds, max_test_group_size)\n","        group_test_starts = range(n_groups - n_splits * group_test_size,\n","                                  n_groups, group_test_size)\n","        for group_test_start in group_test_starts:\n","            train_array = []\n","            test_array = []\n","\n","            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n","            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n","                train_array_tmp = group_dict[train_group_idx]\n","                \n","                train_array = np.sort(np.unique(\n","                                      np.concatenate((train_array,\n","                                                      train_array_tmp)),\n","                                      axis=None), axis=None)\n","\n","            train_end = train_array.size\n"," \n","            for test_group_idx in unique_groups[group_test_start:\n","                                                group_test_start +\n","                                                group_test_size]:\n","                test_array_tmp = group_dict[test_group_idx]\n","                test_array = np.sort(np.unique(\n","                                              np.concatenate((test_array,\n","                                                              test_array_tmp)),\n","                                     axis=None), axis=None)\n","\n","            test_array  = test_array[group_gap:]\n","            \n","            \n","            if self.verbose > 0:\n","                    pass\n","                    \n","            yield [int(i) for i in train_array], [int(i) for i in test_array]"],"metadata":{"id":"NwCZ79B7xNr2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cust_mlp(num_columns, num_labels, hidden_units, dropout_rates, ls = 1e-2, lr = 1e-3):\n","    \n","    inp = tf.keras.layers.Input(shape = (num_columns, ))\n","    x0 = tf.keras.layers.BatchNormalization()(inp)\n","    \n","    encoder = tf.keras.layers.GaussianNoise(dropout_rates[0])(x0)\n","    encoder = tf.keras.layers.Dense(hidden_units[0])(encoder)\n","    encoder = tf.keras.layers.BatchNormalization()(encoder)\n","    encoder = tf.keras.layers.Activation('swish')(encoder)\n","    \n","    decoder = tf.keras.layers.Dropout(dropout_rates[1])(encoder)\n","    decoder = tf.keras.layers.Dense(num_columns, name = 'decoder')(decoder)\n","\n","    x_ae = tf.keras.layers.Dense(hidden_units[1])(decoder)\n","    x_ae = tf.keras.layers.BatchNormalization()(x_ae)\n","    x_ae = tf.keras.layers.Activation('swish')(x_ae)\n","    x_ae = tf.keras.layers.Dropout(dropout_rates[2])(x_ae)\n","\n","    out_ae = tf.keras.layers.Dense(num_labels, activation = 'sigmoid', name = 'ae_action')(x_ae)\n","    \n","    x = tf.keras.layers.Concatenate()([x0, encoder])\n","    x = tf.keras.layers.BatchNormalization()(x)\n","    x = tf.keras.layers.Dropout(dropout_rates[3])(x)\n","    \n","    for i in range(2, len(hidden_units)):\n","        x = tf.keras.layers.Dense(hidden_units[i])(x)\n","        x = tf.keras.layers.BatchNormalization()(x)\n","        x = tf.keras.layers.Activation('swish')(x)\n","        x = tf.keras.layers.Dropout(dropout_rates[i + 2])(x)\n","        \n","    out = tf.keras.layers.Dense(num_labels, activation = 'sigmoid', name = 'action')(x)\n","    \n","    rst = tf.keras.models.Model(inputs = inp, outputs = [decoder, out_ae, out])\n","    rst.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = lr),\n","                  loss = {'decoder': tf.keras.losses.MeanSquaredError(), \n","                          'ae_action': tf.keras.losses.BinaryCrossentropy(label_smoothing = ls),\n","                          'action': tf.keras.losses.BinaryCrossentropy(label_smoothing = ls), \n","                         },\n","                  metrics = {'decoder': tf.keras.metrics.MeanAbsoluteError(name = 'MAE'), \n","                             'ae_action': tf.keras.metrics.AUC(name = 'AUC'), \n","                             'action': tf.keras.metrics.AUC(name = 'AUC'), \n","                            }, \n","                 )\n","    \n","    return rst"],"metadata":{"id":"QK6iydiNxNuK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fcxZPFfbxNwU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HrOUp0-xxNyQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"c2oKqqBxxN0j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Nto_WwSzxN2l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rizoUycLxN5H"},"execution_count":null,"outputs":[]}]}