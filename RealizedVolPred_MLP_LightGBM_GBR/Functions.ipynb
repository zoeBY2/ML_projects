{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPNTKS3kOjTu3mPkHzCzjNx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"uMGhVT1ZufIm"},"outputs":[],"source":["import gc\n","import glob\n","import os\n","import time\n","import traceback\n","from contextlib import contextmanager\n","from joblib import delayed, Parallel\n","\n","from enum import Enum\n","from typing import Dict, List, Optional, Tuple, Union\n","\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from IPython.display import display\n","legpos = 'center left'\n","size = 'medium'\n","loc=(1,0.5)\n","%matplotlib inline\n","\n","import xgboost as xgb\n","from xgboost.sklearn import XGBRegressor\n","\n","import lightgbm as lgb\n","from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn.manifold import TSNE\n","from sklearn.model_selection import GroupKFold\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn.preprocessing import minmax_scale\n","from tqdm import tqdm_notebook as tqdm\n","\n","import random\n","import torch\n","import torch.nn as nn\n","from sklearn.preprocessing import StandardScaler\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm\n","from sklearn.decomposition import PCA\n","# from pytorch_tabnet.metrics import Metric\n","# from pytorch_tabnet.tab_model import TabNetRegressor\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","\n"]},{"cell_type":"code","source":["def log_return(df):\n","    return np.log(df).diff() \n","\n","def realized_volatility(df):\n","    return np.sqrt(np.sum(df**2))\n","\n","def calculate_wap(df):\n","    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n","    b1 = df['bid_size1'] + df['ask_size1']\n","    a2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n","    b2 = df['bid_size2'] + df['ask_size2']\n","    \n","    rst = (a1/b1 + a2/b2)/ 2\n","    \n","    return rst\n","\n","def stock_feature(stock_id, dataType = 'train'):\n","    \n","    book_train = pd.read_parquet(f'./book_{dataType}.parquet/stock_id={stock_id}/')\n","    book_train.sort_values(by=['time_id', 'seconds_in_bucket'])\n","\n","    book_train['range_diff'] = (book_train[['ask_price1', 'ask_price2']].min(axis = 1)\n","                                / book_train[['bid_price1', 'bid_price2']].max(axis = 1)\n","                                - 1)                               \n","    book_train['wap'] = calculate_wap(book_train)\n","\n","    book_train['log_return'] = (book_train.groupby(by = ['time_id'])['wap'].apply(log_return).reset_index(drop = True).fillna(0)\n","                                      )\n","    \n","    rst = pd.merge(\n","        book_train.groupby(by = ['time_id'])['log_return'].agg(realized_volatility).reset_index(),\n","        book_train.groupby(by = ['time_id'], as_index = False)['range_diff'].mean(),\n","        on = ['time_id'],\n","        how = 'left'\n","    )\n","    \n","    rst.insert(0, \"stock_id\", stock_id) \n","    \n","    return rst\n","    \n","def get_dataSet(stock_ids, dataType = 'train'):\n","    \n","    tmp = Parallel(n_jobs=-1)(delayed(stock_feature)(stock_id, dataType) for stock_id in stock_ids)\n","    rst = pd.concat(tmp, ignore_index = True)\n","\n","    return rst"],"metadata":{"id":"IvUm2Kw8WIVC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##### Load data\n","class DataBlock(Enum):\n","    TRAIN = 1\n","    TEST = 2\n","    BOTH = 3\n","\n","\n","def load_stock_data(stock_id: int, directory: str) -> pd.DataFrame:\n","    return pd.read_parquet(os.path.join(DATA_DIR, directory, f'stock_id={stock_id}'))\n","\n","\n","def load_data(stock_id: int, stem: str, block: DataBlock) -> pd.DataFrame:\n","    if block == DataBlock.TRAIN:\n","        return load_stock_data(stock_id, f'{stem}_train.parquet')\n","    elif block == DataBlock.TEST:\n","        return load_stock_data(stock_id, f'{stem}_test.parquet')\n","    else:\n","        return pd.concat([\n","            load_data(stock_id, stem, DataBlock.TRAIN),\n","            load_data(stock_id, stem, DataBlock.TEST)\n","        ]).reset_index(drop=True)\n","\n","def load_book(stock_id: int, block: DataBlock=DataBlock.TRAIN) -> pd.DataFrame:\n","    return load_data(stock_id, 'book', block)\n","\n","\n","def load_trade(stock_id: int, block=DataBlock.TRAIN) -> pd.DataFrame:\n","    return load_data(stock_id, 'trade', block)\n","\n","\n","def calc_wap1(df: pd.DataFrame) -> pd.Series:\n","    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n","    return wap\n","\n","\n","def calc_wap2(df: pd.DataFrame) -> pd.Series:\n","    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n","    return wap\n","\n","\n","def realized_volatility(series):\n","    return np.sqrt(np.sum(series**2))\n","\n","def log_return_df2(series):\n","    return np.log(series).diff(2)\n","\n","\n","def flatten_name(prefix, src_names):\n","    ret = []\n","    for c in src_names:\n","        if c[0] in ['time_id', 'stock_id']:\n","            ret.append(c[0])\n","        else:\n","            ret.append('.'.join([prefix] + list(c)))\n","    return ret\n","\n","\n","def make_book_feature(stock_id, block = DataBlock.TRAIN):\n","    book = load_book(stock_id, block)\n","\n","    book['wap1'] = calc_wap1(book)\n","    book['wap2'] = calc_wap2(book)\n","    book['log_return1'] = book.groupby(['time_id'])['wap1'].apply(log_return)\n","    book['log_return2'] = book.groupby(['time_id'])['wap2'].apply(log_return)\n","    book['log_return_ask1'] = book.groupby(['time_id'])['ask_price1'].apply(log_return)\n","    book['log_return_ask2'] = book.groupby(['time_id'])['ask_price2'].apply(log_return)\n","    book['log_return_bid1'] = book.groupby(['time_id'])['bid_price1'].apply(log_return)\n","    book['log_return_bid2'] = book.groupby(['time_id'])['bid_price2'].apply(log_return)\n","\n","    book['wap_balance'] = abs(book['wap1'] - book['wap2'])\n","    book['price_spread'] = (book['ask_price1'] - book['bid_price1']) / ((book['ask_price1'] + book['bid_price1']) / 2)\n","    book['bid_spread'] = book['bid_price1'] - book['bid_price2']\n","    book['ask_spread'] = book['ask_price1'] - book['ask_price2']\n","    book['total_volume'] = (book['ask_size1'] + book['ask_size2']) + (book['bid_size1'] + book['bid_size2'])\n","    book['volume_imbalance'] = abs((book['ask_size1'] + book['ask_size2']) - (book['bid_size1'] + book['bid_size2']))\n","    \n","    features = {\n","        'seconds_in_bucket': ['count'],\n","        'wap1': [np.sum, np.mean, np.std],\n","        'wap2': [np.sum, np.mean, np.std],\n","        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n","        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n","        'log_return_ask1': [np.sum, realized_volatility, np.mean, np.std],\n","        'log_return_ask2': [np.sum, realized_volatility, np.mean, np.std],\n","        'log_return_bid1': [np.sum, realized_volatility, np.mean, np.std],\n","        'log_return_bid2': [np.sum, realized_volatility, np.mean, np.std],\n","        'wap_balance': [np.sum, np.mean, np.std],\n","        'price_spread':[np.sum, np.mean, np.std],\n","        'bid_spread':[np.sum, np.mean, np.std],\n","        'ask_spread':[np.sum, np.mean, np.std],\n","        'total_volume':[np.sum, np.mean, np.std],\n","        'volume_imbalance':[np.sum, np.mean, np.std]\n","    }\n","    \n","    agg = book.groupby('time_id').agg(features).reset_index(drop=False)\n","    agg.columns = flatten_name('book', agg.columns)\n","    agg['stock_id'] = stock_id\n","    \n","    for time in [450, 300, 150]:\n","        d = book[book['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)\n","        d.columns = flatten_name(f'book_{time}', d.columns)\n","        agg = pd.merge(agg, d, on='time_id', how='left')\n","    return agg\n","\n","\n","def make_trade_feature(stock_id, block = DataBlock.TRAIN):\n","    trade = load_trade(stock_id, block)\n","    trade['log_return'] = trade.groupby('time_id')['price'].apply(log_return)\n","\n","    features = {\n","        'log_return':[realized_volatility],\n","        'seconds_in_bucket':['count'],\n","        'size':[np.sum],\n","        'order_count':[np.mean],\n","    }\n","\n","    agg = trade.groupby('time_id').agg(features).reset_index()\n","    agg.columns = flatten_name('trade', agg.columns)\n","    agg['stock_id'] = stock_id\n","        \n","    for time in [450, 300, 150]:\n","        d = trade[trade['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)\n","        d.columns = flatten_name(f'trade_{time}', d.columns)\n","        agg = pd.merge(agg, d, on='time_id', how='left')\n","    return agg\n","\n","\n","def make_book_feature_v2(stock_id, block = DataBlock.TRAIN):\n","    book = load_book(stock_id, block)\n","\n","    prices = book.set_index('time_id')[['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2']]\n","    time_ids = list(set(prices.index))\n","\n","    ticks = {}\n","    for tid in time_ids:\n","        try:\n","            price_list = prices.loc[tid].values.flatten()\n","            price_diff = sorted(np.diff(sorted(set(price_list))))\n","            ticks[tid] = price_diff[0]\n","        except Exception:\n","            print_trace(f'tid={tid}')\n","            ticks[tid] = np.nan\n","        \n","    dst = pd.DataFrame()\n","    dst['time_id'] = np.unique(book['time_id'])\n","    dst['stock_id'] = stock_id\n","    dst['tick_size'] = dst['time_id'].map(ticks)\n","\n","    return dst\n","\n","\n","def make_features(base, block):\n","    stock_ids = set(base['stock_id'])\n","    with timer('books'):\n","        books = Parallel(n_jobs=-1)(delayed(make_book_feature)(i, block) for i in stock_ids)\n","        book = pd.concat(books)\n","\n","    with timer('trades'):\n","        trades = Parallel(n_jobs=-1)(delayed(make_trade_feature)(i, block) for i in stock_ids)\n","        trade = pd.concat(trades)\n","\n","    with timer('extra features'):\n","        df = pd.merge(base, book, on=['stock_id', 'time_id'], how='left')\n","        df = pd.merge(df, trade, on=['stock_id', 'time_id'], how='left')\n","        #df = make_extra_features(df)\n","\n","    return df\n","\n"],"metadata":{"id":"n3PI4QjX0a-r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Neighbors:\n","    def __init__(self, \n","                 name: str, \n","                 pivot: pd.DataFrame, \n","                 p: float, \n","                 metric: str = 'minkowski', \n","                 metric_params: Optional[Dict] = None, \n","                 exclude_self: bool = False):\n","        self.name = name\n","        self.exclude_self = exclude_self\n","        self.p = p\n","        self.metric = metric\n","        \n","        if metric == 'random':\n","            n_queries = len(pivot)\n","            self.neighbors = np.random.randint(n_queries, size=(n_queries, N_NEIGHBORS_MAX))\n","        else:\n","            nn = NearestNeighbors(\n","                n_neighbors=N_NEIGHBORS_MAX, \n","                p=p, \n","                metric=metric, \n","                metric_params=metric_params\n","            )\n","            nn.fit(pivot)\n","            _, self.neighbors = nn.kneighbors(pivot, return_distance=True)\n","\n","        self.columns = self.index = self.feature_values = self.feature_col = None\n","\n","    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n","        raise NotImplementedError()\n","\n","    def make_nn_feature(self, n=5, agg=np.mean) -> pd.DataFrame:\n","        assert self.feature_values is not None, \"should call rearrange_feature_values beforehand\"\n","\n","        start = 1 if self.exclude_self else 0\n","\n","        pivot_aggs = pd.DataFrame(\n","            agg(self.feature_values[start:n,:,:], axis=0), \n","            columns=self.columns, \n","            index=self.index\n","        )\n","\n","        dst = pivot_aggs.unstack().reset_index()\n","        dst.columns = ['stock_id', 'time_id', f'{self.feature_col}_nn{n}_{self.name}_{agg.__name__}']\n","        return dst\n","\n","class StockIdNeighbors(Neighbors):\n","    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n","        \"\"\"stock-id based nearest neighbor features\"\"\"\n","        feature_pivot = df.pivot('time_id', 'stock_id', feature_col)\n","        feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n","\n","        feature_values = np.zeros((N_NEIGHBORS_MAX, *feature_pivot.shape))\n","\n","        for i in range(N_NEIGHBORS_MAX):\n","            feature_values[i, :, :] += feature_pivot.values[:, self.neighbors[:, i]]\n","\n","        self.columns = list(feature_pivot.columns)\n","        self.index = list(feature_pivot.index)\n","        self.feature_values = feature_values\n","        self.feature_col = feature_col\n","        \n","    def __repr__(self) -> str:\n","        return f\"stock-id NN (name={self.name}, metric={self.metric}, p={self.p})\"\n"],"metadata":{"id":"cu3L9AAdOuML"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_nearest_neighbor_feature(df) -> pd.DataFrame:\n","    df2 = df.copy()\n","    print(df2.shape)\n","\n","    feature_cols_stock = {\n","        'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n","        'trade.seconds_in_bucket.count': [np.mean],\n","        'trade.tau': [np.mean],\n","        'trade_150.tau': [np.mean],\n","        'book.tau': [np.mean],\n","        'trade.size.sum': [np.mean],\n","        'book.seconds_in_bucket.count': [np.mean],\n","    }\n","    \n","    feature_cols = {\n","        'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n","        'real_price': [np.max, np.mean, np.min],\n","        'trade.seconds_in_bucket.count': [np.mean],\n","        'trade.tau': [np.mean],\n","        'trade.size.sum': [np.mean],\n","        'book.seconds_in_bucket.count': [np.mean],\n","        'trade_150.tau_nn20_stock_vol_l1_mean': [np.mean],\n","        'trade.size.sum_nn20_stock_vol_l1_mean': [np.mean],\n","    }\n","\n","    time_id_neigbor_sizes = [3, 5, 10, 20, 40]\n","    time_id_neigbor_sizes_vol = [2, 3, 5, 10, 20, 40]\n","    stock_id_neighbor_sizes = [10, 20, 40]\n","\n","    ndf: Optional[pd.DataFrame] = None\n","\n","    def _add_ndf(ndf: Optional[pd.DataFrame], dst: pd.DataFrame) -> pd.DataFrame:\n","        if ndf is None:\n","            return dst\n","        else:\n","            ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n","            return ndf\n","\n","    # neighbor stock_id\n","    for feature_col in feature_cols_stock.keys():\n","        try:\n","            if feature_col not in df2.columns:\n","                print(f\"column {feature_col} is skipped\")\n","                continue\n","\n","            if not stock_id_neighbors:\n","                continue\n","\n","            for nn in stock_id_neighbors:\n","                nn.rearrange_feature_values(df2, feature_col)\n","\n","            for agg in feature_cols_stock[feature_col]:\n","                for n in stock_id_neighbor_sizes:\n","                    try:\n","                        for nn in stock_id_neighbors:\n","                            dst = nn.make_nn_feature(n, agg)\n","                            ndf = _add_ndf(ndf, dst)\n","                    except Exception:\n","                        print_trace('stock-id nn')\n","                        pass\n","        except Exception:\n","            print_trace('stock-id nn')\n","            pass\n","\n","    if ndf is not None:\n","        df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n","    ndf = None\n","\n","    print(df2.shape)\n","\n","    # neighbor time_id\n","    for feature_col in feature_cols.keys():\n","        try:\n","            if not USE_PRICE_NN_FEATURES and feature_col == 'real_price':\n","                continue\n","            if feature_col not in df2.columns:\n","                print(f\"column {feature_col} is skipped\")\n","                continue\n","\n","            for nn in time_id_neighbors:\n","                nn.rearrange_feature_values(df2, feature_col)\n","\n","            if 'volatility' in feature_col:\n","                time_id_ns = time_id_neigbor_sizes_vol\n","            else:\n","                time_id_ns = time_id_neigbor_sizes\n","\n","            for agg in feature_cols[feature_col]:\n","                for n in time_id_ns:\n","                    try:\n","                        for nn in time_id_neighbors:\n","                            dst = nn.make_nn_feature(n, agg)\n","                            ndf = _add_ndf(ndf, dst)\n","                    except Exception:\n","                        print_trace('time-id nn')\n","                        pass\n","        except Exception:\n","            print_trace('time-id nn')\n","\n","    if ndf is not None:\n","        df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n","\n","    # features further derived from nearest neighbor features\n","    try:\n","        if USE_PRICE_NN_FEATURES:\n","            for sz in time_id_neigbor_sizes:\n","                denominator = f\"real_price_nn{sz}_time_price_c\"\n","\n","                df2[f'real_price_rankmin_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amin\"]\n","                df2[f'real_price_rankmax_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amax\"]\n","                df2[f'real_price_rankmean_{sz}'] = df2['real_price'] / df2[f\"{denominator}_mean\"]\n","\n","            for sz in time_id_neigbor_sizes_vol:\n","                denominator = f\"book.log_return1.realized_volatility_nn{sz}_time_price_c\"\n","\n","                df2[f'vol_rankmin_{sz}'] = \\\n","                    df2['book.log_return1.realized_volatility'] / df2[f\"{denominator}_amin\"]\n","                df2[f'vol_rankmax_{sz}'] = \\\n","                    df2['book.log_return1.realized_volatility'] / df2[f\"{denominator}_amax\"]\n","\n","        price_cols = [c for c in df2.columns if 'real_price' in c and 'rank' not in c]\n","        for c in price_cols:\n","            del df2[c]\n","\n","        if USE_PRICE_NN_FEATURES:\n","            for sz in time_id_neigbor_sizes_vol:\n","                tgt = f'book.log_return1.realized_volatility_nn{sz}_time_price_m_mean'\n","                df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n","    except Exception:\n","        print_trace('nn features')\n","\n","    return df2"],"metadata":{"id":"lwT73R9TUHDy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def rmspe(y_true, y_pred):\n","    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n","\n","\n","def feval_RMSPE(preds, train_data):\n","    labels = train_data.get_label()\n","    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n","\n","def plot_importance(cvbooster, figsize=(10, 10)):\n","    raw_importances = cvbooster.feature_importance(importance_type='gain')\n","    feature_name = cvbooster.boosters[0].feature_name()\n","    importance_df = pd.DataFrame(data=raw_importances,\n","                                 columns=feature_name)\n","    # order by average importance across folds\n","    sorted_indices = importance_df.mean(axis=0).sort_values(ascending=False).index\n","    sorted_importance_df = importance_df.loc[:, sorted_indices]\n","    # plot top-n\n","    PLOT_TOP_N = 50\n","    plot_cols = sorted_importance_df.columns[:PLOT_TOP_N]\n","    _, ax = plt.subplots(figsize=figsize)\n","    ax.grid()\n","    ax.set_xscale('log')\n","    ax.set_ylabel('Feature')\n","    ax.set_xlabel('Importance')\n","    sns.boxplot(data=sorted_importance_df[plot_cols],\n","                orient='h',\n","                ax=ax)\n","    plt.show()\n","\n","\n","def get_X(df_src):\n","    cols = [c for c in df_src.columns if c not in ['time_id', 'target', 'tick_size']]\n","    return df_src[cols]\n","\n","\n","class EnsembleModel:\n","    def __init__(self, models: List[lgb.Booster], weights: Optional[List[float]] = None):\n","        self.models = models\n","        self.weights = weights\n","\n","        features = list(self.models[0].feature_name())\n","\n","        for m in self.models[1:]:\n","            assert features == list(m.feature_name())\n","\n","    def predict(self, x):\n","        predicted = np.zeros((len(x), len(self.models)))\n","\n","        for i, m in enumerate(self.models):\n","            w = self.weights[i] if self.weights is not None else 1\n","            predicted[:, i] = w * m.predict(x)\n","\n","        ttl = np.sum(self.weights) if self.weights is not None else len(self.models)\n","        return np.sum(predicted, axis=1) / ttl\n","\n","    def feature_name(self) -> List[str]:\n","        return self.models[0].feature_name()"],"metadata":{"id":"TajIImMmUatV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["null_check_cols = [\n","    'book.log_return1.realized_volatility',\n","    'book_150.log_return1.realized_volatility',\n","    'book_300.log_return1.realized_volatility',\n","    'book_450.log_return1.realized_volatility',\n","    'trade.log_return.realized_volatility',\n","    'trade_150.log_return.realized_volatility',\n","    'trade_300.log_return.realized_volatility',\n","    'trade_450.log_return.realized_volatility'\n","]\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","\n","def rmspe_metric(y_true, y_pred):\n","    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n","    return rmspe\n","\n","\n","def rmspe_loss(y_true, y_pred):\n","    rmspe = torch.sqrt(torch.mean(torch.square((y_true - y_pred) / y_true)))\n","    return rmspe\n","\n","def RMSPELoss_Tabnet(y_pred, y_true):\n","    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()\n","\n","\n","class AverageMeter:\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","\n","    def __init__(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","class TabularDataset(Dataset):\n","    def __init__(self, x_num: np.ndarray, x_cat: np.ndarray, y: Optional[np.ndarray]):\n","        super().__init__()\n","        self.x_num = x_num\n","        self.x_cat = x_cat\n","        self.y = y\n","\n","    def __len__(self):\n","        return len(self.x_num)\n","\n","    def __getitem__(self, idx):\n","        if self.y is None:\n","            return self.x_num[idx], torch.LongTensor(self.x_cat[idx])\n","        else:\n","            return self.x_num[idx], torch.LongTensor(self.x_cat[idx]), self.y[idx]\n","\n","\n","class MLP(nn.Module):\n","    def __init__(self,\n","                 src_num_dim: int,\n","                 n_categories: List[int],\n","                 dropout: float = 0.0,\n","                 hidden: int = 50,\n","                 emb_dim: int = 10,\n","                 dropout_cat: float = 0.2,\n","                 bn: bool = False):\n","        super().__init__()\n","\n","        self.embs = nn.ModuleList([\n","            nn.Embedding(x, emb_dim) for x in n_categories])\n","        self.cat_dim = emb_dim * len(n_categories)\n","        self.dropout_cat = nn.Dropout(dropout_cat)\n","\n","        if bn:\n","            self.sequence = nn.Sequential(\n","                nn.Linear(src_num_dim + self.cat_dim, hidden),\n","                nn.Dropout(dropout),\n","                nn.BatchNorm1d(hidden),\n","                nn.ReLU(),\n","                nn.Linear(hidden, hidden),\n","                nn.Dropout(dropout),\n","                nn.BatchNorm1d(hidden),\n","                nn.ReLU(),\n","                nn.Linear(hidden, 1)\n","            )\n","        else:\n","            self.sequence = nn.Sequential(\n","                nn.Linear(src_num_dim + self.cat_dim, hidden),\n","                nn.Dropout(dropout),\n","                nn.ReLU(),\n","                nn.Linear(hidden, hidden),\n","                nn.Dropout(dropout),\n","                nn.ReLU(),\n","                nn.Linear(hidden, 1)\n","            )\n","\n","    def forward(self, x_num, x_cat):\n","        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n","        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n","        x_all = torch.cat([x_num, x_cat_emb], 1)\n","        x = self.sequence(x_all)\n","        return torch.squeeze(x)\n","\n","\n","class CNN(nn.Module):\n","    def __init__(self,\n","                 num_features: int,\n","                 hidden_size: int,\n","                 n_categories: List[int],\n","                 emb_dim: int = 10,\n","                 dropout_cat: float = 0.2,\n","                 channel_1: int = 256,\n","                 channel_2: int = 512,\n","                 channel_3: int = 512,\n","                 dropout_top: float = 0.1,\n","                 dropout_mid: float = 0.3,\n","                 dropout_bottom: float = 0.2,\n","                 weight_norm: bool = True,\n","                 two_stage: bool = True,\n","                 celu: bool = True,\n","                 kernel1: int = 5,\n","                 leaky_relu: bool = False):\n","        super().__init__()\n","\n","        num_targets = 1\n","\n","        cha_1_reshape = int(hidden_size / channel_1)\n","        cha_po_1 = int(hidden_size / channel_1 / 2)\n","        cha_po_2 = int(hidden_size / channel_1 / 2 / 2) * channel_3\n","\n","        self.cat_dim = emb_dim * len(n_categories)\n","        self.cha_1 = channel_1\n","        self.cha_2 = channel_2\n","        self.cha_3 = channel_3\n","        self.cha_1_reshape = cha_1_reshape\n","        self.cha_po_1 = cha_po_1\n","        self.cha_po_2 = cha_po_2\n","        self.two_stage = two_stage\n","\n","        self.expand = nn.Sequential(\n","            nn.BatchNorm1d(num_features + self.cat_dim),\n","            nn.Dropout(dropout_top),\n","            nn.utils.weight_norm(nn.Linear(num_features + self.cat_dim, hidden_size), dim=None),\n","            nn.CELU(0.06) if celu else nn.ReLU()\n","        )\n","\n","        def _norm(layer, dim=None):\n","            return nn.utils.weight_norm(layer, dim=dim) if weight_norm else layer\n","\n","        self.conv1 = nn.Sequential(\n","            nn.BatchNorm1d(channel_1),\n","            nn.Dropout(dropout_top),\n","            _norm(nn.Conv1d(channel_1, channel_2, kernel_size=kernel1, stride=1, padding=kernel1 // 2, bias=False)),\n","            nn.ReLU(),\n","            nn.AdaptiveAvgPool1d(output_size=cha_po_1),\n","            nn.BatchNorm1d(channel_2),\n","            nn.Dropout(dropout_top),\n","            _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n","            nn.ReLU()\n","        )\n","\n","        if self.two_stage:\n","            self.conv2 = nn.Sequential(\n","                nn.BatchNorm1d(channel_2),\n","                nn.Dropout(dropout_mid),\n","                _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n","                nn.ReLU(),\n","                nn.BatchNorm1d(channel_2),\n","                nn.Dropout(dropout_bottom),\n","                _norm(nn.Conv1d(channel_2, channel_3, kernel_size=5, stride=1, padding=2, bias=True)),\n","                nn.ReLU()\n","            )\n","\n","        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n","\n","        self.flt = nn.Flatten()\n","\n","        if leaky_relu:\n","            self.dense = nn.Sequential(\n","                nn.BatchNorm1d(cha_po_2),\n","                nn.Dropout(dropout_bottom),\n","                _norm(nn.Linear(cha_po_2, num_targets), dim=0),\n","                nn.LeakyReLU()\n","            )\n","        else:\n","            self.dense = nn.Sequential(\n","                nn.BatchNorm1d(cha_po_2),\n","                nn.Dropout(dropout_bottom),\n","                _norm(nn.Linear(cha_po_2, num_targets), dim=0)\n","            )\n","\n","        self.embs = nn.ModuleList([nn.Embedding(x, emb_dim) for x in n_categories])\n","        self.cat_dim = emb_dim * len(n_categories)\n","        self.dropout_cat = nn.Dropout(dropout_cat)\n","\n","    def forward(self, x_num, x_cat):\n","        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n","        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n","        x = torch.cat([x_num, x_cat_emb], 1)\n","\n","        x = self.expand(x)\n","\n","        x = x.reshape(x.shape[0], self.cha_1, self.cha_1_reshape)\n","\n","        x = self.conv1(x)\n","\n","        if self.two_stage:\n","            x = self.conv2(x) * x\n","\n","        x = self.max_po_c2(x)\n","        x = self.flt(x)\n","        x = self.dense(x)\n","\n","        return torch.squeeze(x)\n","\n","\n","def preprocess_nn(\n","        X: pd.DataFrame,\n","        scaler: Optional[StandardScaler] = None,\n","        scaler_type: str = 'standard',\n","        n_pca: int = -1,\n","        na_cols: bool = True):\n","    if na_cols:\n","        #for c in X.columns:\n","        for c in null_check_cols:\n","            if c in X.columns:\n","                X[f\"{c}_isnull\"] = X[c].isnull().astype(int)\n","\n","    cat_cols = [c for c in X.columns if c in ['time_id', 'stock_id']]\n","    num_cols = [c for c in X.columns if c not in cat_cols]\n","\n","    X_num = X[num_cols].values.astype(np.float32)\n","    X_cat = np.nan_to_num(X[cat_cols].values.astype(np.int32))\n","\n","    def _pca(X_num_):\n","        if n_pca > 0:\n","            pca = PCA(n_components=n_pca, random_state=0)\n","            return pca.fit_transform(X_num)\n","        return X_num\n","\n","    if scaler is None:\n","        scaler = StandardScaler()\n","        X_num = scaler.fit_transform(X_num)\n","        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n","        return _pca(X_num), X_cat, cat_cols, scaler\n","    else:\n","        X_num = scaler.transform(X_num) \n","        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n","        return _pca(X_num), X_cat, cat_cols\n","\n","\n","def train_epoch(data_loader: DataLoader,\n","                model: nn.Module,\n","                optimizer,\n","                scheduler,\n","                device,\n","                clip_grad: float = 1.5):\n","    model.train()\n","    losses = AverageMeter()\n","    step = 0\n","\n","    for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Training'):\n","        batch_size = x_num.size(0)\n","        x_num = x_num.to(device, dtype=torch.float)\n","        x_cat = x_cat.to(device)\n","        y = y.to(device, dtype=torch.float)\n","\n","        loss = rmspe_loss(y, model(x_num, x_cat))\n","        losses.update(loss.detach().cpu().numpy(), batch_size)\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        if scheduler is not None:\n","            scheduler.step()\n","\n","        step += 1\n","\n","    return losses.avg\n","\n","\n","def evaluate(data_loader: DataLoader, model, device):\n","    model.eval()\n","\n","    losses = AverageMeter()\n","\n","    final_targets = []\n","    final_outputs = []\n","\n","    with torch.no_grad():\n","        for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n","            batch_size = x_num.size(0)\n","            x_num = x_num.to(device, dtype=torch.float)\n","            x_cat = x_cat.to(device)\n","            y = y.to(device, dtype=torch.float)\n","\n","            with torch.no_grad():\n","                output = model(x_num, x_cat)\n","\n","            loss = rmspe_loss(y, output)\n","            # record loss\n","            losses.update(loss.detach().cpu().numpy(), batch_size)\n","\n","            targets = y.detach().cpu().numpy()\n","            output = output.detach().cpu().numpy()\n","\n","            final_targets.append(targets)\n","            final_outputs.append(output)\n","\n","    final_targets = np.concatenate(final_targets)\n","    final_outputs = np.concatenate(final_outputs)\n","\n","    try:\n","        metric = rmspe_metric(final_targets, final_outputs)\n","    except:\n","        metric = None\n","\n","    return final_outputs, final_targets, losses.avg, metric\n","\n","\n","def predict_nn(X: pd.DataFrame,\n","               model: Union[List[MLP], MLP],\n","               scaler: StandardScaler,\n","               device,\n","               ensemble_method='mean'):\n","    if not isinstance(model, list):\n","        model = [model]\n","\n","    for m in model:\n","        m.eval()\n","    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n","    valid_dataset = TabularDataset(X_num, X_cat, None)\n","    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n","                                               batch_size=512,\n","                                               shuffle=False,\n","                                               num_workers=4)\n","\n","    final_outputs = []\n","\n","    with torch.no_grad():\n","        for x_num, x_cat in tqdm(valid_loader, position=0, leave=True, desc='Evaluating'):\n","            x_num = x_num.to(device, dtype=torch.float)\n","            x_cat = x_cat.to(device)\n","\n","            outputs = []\n","            with torch.no_grad():\n","                for m in model:\n","                    output = m(x_num, x_cat)\n","                    outputs.append(output.detach().cpu().numpy())\n","\n","            if ensemble_method == 'median':\n","                pred = np.nanmedian(np.array(outputs), axis=0)\n","            else:\n","                pred = np.array(outputs).mean(axis=0)\n","            final_outputs.append(pred)\n","\n","    final_outputs = np.concatenate(final_outputs)\n","    return final_outputs\n"],"metadata":{"id":"piTcADR2O5qN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@contextmanager\n","def timer(name: str):\n","    s = time.time()\n","    yield\n","    elapsed = time.time() - s\n","    print(f'[{name}] {elapsed: .3f}sec')\n","    \n","def print_trace(name: str = ''):\n","    print(f'ERROR RAISED IN {name or \"anonymous\"}')\n","    print(traceback.format_exc())"],"metadata":{"id":"3h77Jm_g0bBW"},"execution_count":null,"outputs":[]}]}