{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('float.format','{:f}'.format)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "pd.set_option('display.max_rows',999)\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import math \n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legpos = 'center left'\n",
    "size = 'medium'\n",
    "loc=(1,0.5)\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set()\n",
    "\n",
    "def visualization(df, x, y, figsize=(12,3), hue=None, scatter=False, dist=False, cust_col='Set2', title='' ,xlabel='', ylabel='', rotation_angel=90):\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    if (scatter):\n",
    "        ax = sns.scatterplot(x=x,y=y,data=df,hue=hue,palette=cust_col)\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.xticks(rotation=rotation_angel)\n",
    "        ax.set(xlabel=xlabel, ylabel=ylabel)\n",
    "#     elif (dist):\n",
    "#         ax = sns.displot(x=x,y=y,data=df,hue=hue,palette=cust_col)\n",
    "\n",
    "#         plt.title(title)\n",
    "#         plt.xticks(rotation=rotation_angel)\n",
    "#         ax.set(xlabel=xlabel, ylabel=ylabel)\n",
    "\n",
    "    elif (hue != None):\n",
    "        ax = sns.lineplot(x=x,y=y,data=df,hue=hue,palette=cust_col)\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.xticks(rotation=rotation_angel)\n",
    "        plt.legend(loc=legpos,bbox_to_anchor=loc,fontsize=size)\n",
    "        ax.set(xlabel=xlabel, ylabel=ylabel)\n",
    "    else:\n",
    "        ax = sns.lineplot(x=x,y=y,data=df,palette=cust_col)\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.xticks(rotation=rotation_angel)\n",
    "        ax.set(xlabel=xlabel, ylabel=ylabel)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def visualization_3d(df, x, y, z,figsize=(8,8), title='' ,xlabel='', ylabel='', zlabel=''):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.scatter3D(df[x], df[y], df[z], cmap='Greens')\n",
    "    plt.title(title)\n",
    "    ax.set(xlabel=xlabel, ylabel=ylabel,zlabel=zlabel)\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(df2,df3,df4):\n",
    "    tmp2 = df2.copy()\n",
    "    tmp3 = df3.copy()\n",
    "    tmp4 = df4.copy()\n",
    "    \n",
    "    tmp2.loc[:,'type'] = 1\n",
    "    tmp3.loc[:,'type'] = 2\n",
    "    tmp4.loc[:,'type'] = 3\n",
    "    \n",
    "\n",
    "    tmp = pd.concat([tmp2,tmp3,tmp4])\n",
    "\n",
    "    tmp = tmp[['X1', 'X2', 'X3','type']].reset_index(drop=True)\n",
    "\n",
    "    # tmp = tmp.copy()\n",
    "    tmp.loc[:,'X1X2'] = tmp.X1 * tmp.X2\n",
    "    tmp.loc[:,'X1X3'] = tmp.X1 * tmp.X3\n",
    "    tmp.loc[:,'X2X3'] = tmp.X2 * tmp.X3\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "def classification_accuracy(df):\n",
    "    df = df.copy()\n",
    "    df.loc[:,'predict_type'] = 1\n",
    "    df.loc[(df['X1X2']<0) & (df['X1X3']>0),'predict_type'] = 2\n",
    "    accuracy = (df['type'] == df['predict_type']).sum() / df.shape[0]\n",
    "    \n",
    "    return accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_cleaning(df,start_date,end_date,dates_exclude=['2022-08-05']):\n",
    "    df = df.copy()\n",
    "    \n",
    "    ##### data type \n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df.columns = df.columns.astype(str)\n",
    "    ##### available dates \n",
    "    df = df.loc[(df.index >= start_date) & (df.index <= end_date) & (~df.index.isin(dates_exclude))]\n",
    "    \n",
    "    return df \n",
    "\n",
    "def display_features(df,feature='cap',scatter=False):\n",
    "    display(feature + ' start date: '+str(df.index.min()))\n",
    "    display(feature + ' end date: '+str(df.index.max()))\n",
    "    tmp = df.notnull().sum(axis=1).to_frame().rename(columns={0:'cnt'})\n",
    "    if not scatter:\n",
    "        visualization(df=None, x=tmp.index,y=tmp.cnt,title='Number of daily available securities - '+feature)\n",
    "    else:\n",
    "        visualization(df=None, x=tmp.index,y=tmp.cnt,title='Number of daily available securities - '+feature, scatter=True)\n",
    "        \n",
    "def whether_in_universe(df):\n",
    "    isin_univ = df.copy()\n",
    "    isin_univ = isin_univ.fillna(0)\n",
    "    isin_univ[isin_univ != 0] = 1.0\n",
    "    isin_univ = isin_univ.astype('int')\n",
    "    return isin_univ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ttm_gross_sales(financial, current_date, symbol, lag_quarter):   \n",
    "    query_quarters = 4 + lag_quarter\n",
    "    \n",
    "    financial_symbol = financial[financial['symbol']==symbol]\n",
    "    financial_segment = financial_symbol[financial_symbol['Date']<current_date].sort_values(by='QuarterNum', ascending=True).set_index('QuarterNum')\n",
    "    if financial_segment.shape[0] == 0:\n",
    "        return np.nan\n",
    "\n",
    "    end_quarter_num = financial_segment.index[-1] - lag_quarter\n",
    "    start_quarter_num = end_quarter_num - 3\n",
    "\n",
    "    finalcial_cal_period = financial_segment.loc[start_quarter_num:end_quarter_num]\n",
    "    if finalcial_cal_period.shape[0] > 0:\n",
    "        sales_ttm = finalcial_cal_period['GrossSales'].mean() * 4\n",
    "    else:\n",
    "        sales_ttm = np.nan\n",
    "    return sales_ttm    \n",
    "    \n",
    "\n",
    "def get_ttm_net_income(financial, current_date, symbol, lag_quarter):\n",
    "    query_quarters = 4 + lag_quarter\n",
    "    \n",
    "    financial_symbol = financial[financial['symbol']==symbol]\n",
    "    financial_segment = financial_symbol[financial_symbol['Date']<current_date].sort_values(by='QuarterNum', ascending=True).set_index('QuarterNum')\n",
    "    if financial_segment.shape[0] == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # period for filling missing values\n",
    "    end_quarter_num = financial_segment.index[-1]\n",
    "    start_quarter_num = end_quarter_num - query_quarters\n",
    "\n",
    "    finalcial_cal_period = financial_segment.loc[start_quarter_num:end_quarter_num]\n",
    "\n",
    "    # fill missing\n",
    "    net_income_table = pd.DataFrame(index=range(start_quarter_num, end_quarter_num+1), columns=['FiscalQuarter', 'NetIncome'])\n",
    "    net_income_table.loc[finalcial_cal_period.index, 'FiscalQuarter'] = finalcial_cal_period['FiscalQuarter']\n",
    "    net_income_table.loc[finalcial_cal_period.index, 'NetIncome'] = finalcial_cal_period['NetIncome']\n",
    "    net_income_table['NI_per_quarter'] = (net_income_table['NetIncome'] / net_income_table['FiscalQuarter']).fillna(method='ffill').fillna(method='bfill')\n",
    "    net_income_table['NI_noncumulative'] = net_income_table['NetIncome'].diff()\n",
    "    net_income_table.loc[net_income_table['FiscalQuarter']==1, 'NI_noncumulative'] = net_income_table.loc[net_income_table['FiscalQuarter']==1, 'NetIncome']\n",
    "    net_income_table.loc[pd.isnull(net_income_table['NI_noncumulative']), 'NI_noncumulative'] = net_income_table.loc[pd.isnull(net_income_table['NI_noncumulative']), 'NI_per_quarter']\n",
    "\n",
    "    # period for calculation ttm\n",
    "    end_quarter_num = financial_segment.index[-1] - lag_quarter\n",
    "    start_quarter_num = end_quarter_num - 3\n",
    "\n",
    "    ni_ttm = net_income_table.loc[start_quarter_num:end_quarter_num]['NI_noncumulative'].sum()\n",
    "    return ni_ttm\n",
    "\n",
    "def fiscal_report_dates(financial,symbol):\n",
    "    return financial.loc[financial['symbol']==symbol, 'Date'].values\n",
    "\n",
    "def zscore(x, window):\n",
    "    r = x.rolling(window=window)\n",
    "    m = r.mean()\n",
    "    s = r.std()\n",
    "    z = (x-m)/s\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(df,ori_col = ['cp', 'op', 'adv', 'cap', 'holding_indicator',\n",
    "       'technical_indicator', 'net_income_growth', 'profit_margin', 'pe_ratio',\n",
    "       'ps_ratio']):\n",
    "    \n",
    "    ##### get industry avg\n",
    "    industry_avg = df.reset_index().groupby(['date','sector']).mean()\n",
    "    ### rename column \n",
    "    industry_avg = industry_avg[ori_col].reset_index()\n",
    "    for col in ori_col:\n",
    "        industry_avg = industry_avg.rename(columns={col:col+'_sector'})\n",
    "        \n",
    "    ##### merge industry avg \n",
    "    df = df.reset_index().merge(industry_avg, how='left',left_on=['date','sector'],right_on=['date','sector'])\n",
    "    \n",
    "    ##### fill missing values \n",
    "    for col in ori_col:\n",
    "        df.loc[df[col].isnull(),col]=df.loc[df[col].isnull(),col+'_sector']\n",
    "    \n",
    "    df = df[['date','variable','sector']+ori_col].set_index(['date','variable']).sort_index()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_features_df():\n",
    "    sector_ind = sector.reset_index().rename(columns={'index':'date'}).melt(id_vars='date').set_index(['date','variable']).rename(columns={'value':'sector'})\n",
    "\n",
    "    cp_tmp = cp.reset_index().rename(columns={'index':'date'}).melt(id_vars='date').set_index(['date','variable']).rename(columns={'value':'cp'})\n",
    "    op_tmp = op.reset_index().rename(columns={'index':'date'}).melt(id_vars='date').set_index(['date','variable']).rename(columns={'value':'op'})\n",
    "    adv_tmp = adv.reset_index().rename(columns={'index':'date'}).melt(id_vars='date').set_index(['date','variable']).rename(columns={'value':'adv'})\n",
    "    cap_tmp = cap.reset_index().rename(columns={'index':'date'}).melt(id_vars='date').set_index(['date','variable']).rename(columns={'value':'cap'})\n",
    "\n",
    "    holding_indicator_tmp = holding_indicator.reset_index().rename(columns={'index':'date'}).melt(id_vars='date').set_index(['date','variable']).rename(columns={'value':'holding_indicator'})\n",
    "    technical_indicator_tmp = technical_indicator.reset_index().rename(columns={'index':'date'}).melt(id_vars='date').set_index(['date','variable']).rename(columns={'value':'technical_indicator'})\n",
    "\n",
    "    net_income_growth_tmp = net_income_growth.reset_index().rename(columns={'index':'date'}).melt(id_vars='date').set_index(['date','variable']).rename(columns={'value':'net_income_growth'})\n",
    "    profit_margin_tmp = profit_margin.reset_index().rename(columns={'index':'date'}).melt(id_vars='date').set_index(['date','variable']).rename(columns={'value':'profit_margin'})\n",
    "    pe_ratio_tmp = pe_ratio.reset_index().rename(columns={'index':'date'}).melt(id_vars='date').set_index(['date','variable']).rename(columns={'value':'pe_ratio'})\n",
    "    ps_ratio_tmp = ps_ratio.reset_index().rename(columns={'index':'date'}).melt(id_vars='date').set_index(['date','variable']).rename(columns={'value':'ps_ratio'})\n",
    "\n",
    "    \n",
    "    raw_features = sector_ind.merge(cp_tmp,left_index=True, right_index=True, how='left')\n",
    "    raw_features = raw_features.merge(op_tmp,left_index=True, right_index=True, how='left')\n",
    "    raw_features = raw_features.merge(adv_tmp,left_index=True, right_index=True, how='left')\n",
    "    raw_features = raw_features.merge(cap_tmp,left_index=True, right_index=True, how='left')\n",
    "\n",
    "    raw_features = raw_features.merge(holding_indicator_tmp,left_index=True, right_index=True, how='left')\n",
    "    raw_features = raw_features.merge(technical_indicator_tmp,left_index=True, right_index=True, how='left')\n",
    "\n",
    "    raw_features = raw_features.merge(net_income_growth_tmp,left_index=True, right_index=True, how='left')\n",
    "    raw_features = raw_features.merge(profit_margin_tmp,left_index=True, right_index=True, how='left')\n",
    "    raw_features = raw_features.merge(pe_ratio_tmp,left_index=True, right_index=True, how='left')\n",
    "    raw_features = raw_features.merge(ps_ratio_tmp,left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    return raw_features\n",
    "\n",
    "\n",
    "def feature_in_univ(df, univ):\n",
    "    rst = df.reset_index().rename(columns={'index':'date'}).melt(id_vars='date').set_index(['date','variable'])\n",
    "    rst = rst.loc[rst.index.isin(univ.index)]\n",
    "    return rst\n",
    "\n",
    "\n",
    "def universe_selection(df,df2,size=10000,isfilter=False):\n",
    "    df2 = df2[df2>size].copy()\n",
    "    if isfilter:\n",
    "        df2 = df2.reset_index().rename(columns={'index':'date'}).melt(id_vars='date').set_index(['date','variable'])\n",
    "        df1 = df.copy()\n",
    "        df = df.reset_index().rename(columns={'index':'date'}).melt(id_vars='date').set_index(['date','variable']).copy()\n",
    "        df = df.loc[df.index.isin(df2.index)]\n",
    "        df = df[['value']].reset_index().pivot(index='date',columns='variable',values='value')\n",
    "        df = df[df1.columns]\n",
    "    rst = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "    tmp = df.copy()\n",
    "\n",
    "    for row in tmp.index:\n",
    "        top_300 = tmp.loc[row,:].sort_values(ascending=False).iloc[:300].index.to_list()\n",
    "        rst.loc[row,top_300] = 1\n",
    "    return rst\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_betas(univ_brtn: pd.DataFrame):\n",
    "   \n",
    "    ##### get average return for each date\n",
    "    avg_rtn = univ_brtn.groupby(level='date')['bd1'].mean().reset_index().rename(columns={'bd1':'avg_rtn'})\n",
    "    ##### get securities included (500)\n",
    "    sec_list = univ_brtn.index.unique('sector')\n",
    "    ##### for each security, calculate beta\n",
    "    rst = pd.DataFrame(columns=['beta'])\n",
    "    for sec in sec_list:\n",
    "        tmp = univ_brtn.loc[univ_brtn.index.get_level_values('sector') == sec].reset_index()\n",
    "        sec_rtn = tmp['bd1'].values\n",
    "        rtn = avg_rtn.merge(tmp, how='right',left_on='date',right_on='date')\n",
    "        mkt_rtn = rtn['avg_rtn'].values\n",
    "        rst.loc[sec,'beta'] = np.cov(sec_rtn,mkt_rtn)[0,1]/np.var(mkt_rtn)\n",
    "\n",
    "    return rst['beta'].astype(float)\n",
    "\n",
    "def adjust_market_impact(data,univ,dt,signal,col='momentum'):\n",
    "   \n",
    "    ##### adjust for market impacts - beta \n",
    "    ### get beta \n",
    "    beta = get_betas(univ_brtn=data)\n",
    "    \n",
    "    ##### adjust for sector impacts \n",
    "    ### create dummy variables for sector\n",
    "    sector = univ.loc[(univ.index.get_level_values('date')==dt)][['sector']]\n",
    "    sector = pd.get_dummies(data=sector['variable'],prefix='sector').droplevel('date')\n",
    "    \n",
    "    ##### perform linear regression to adjust market and sector impacts \n",
    "    X = pd.concat([beta,sector], axis=1)\n",
    "    adj_signal = signal - LinearRegression().fit(X, signal).predict(X)\n",
    "    \n",
    "    adj_signal.name = col\n",
    "    # another way to adjust mkt & sector impacts can be: perform stepwise regression\n",
    "    # adjust mkt impacts first \n",
    "    # then adjust for sector impacts \n",
    "    return adj_signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signals(df,features_clean):\n",
    "    ######## return \n",
    "    close_rtn = df.pct_change(1)\n",
    "    fwd_rtn_20 = df.pct_change(20).shift(-1)\n",
    "\n",
    "    ##### return ts mean \n",
    "    rtn_1w = close_rtn.rolling(window=5).mean()\n",
    "    rtn_1m = close_rtn.rolling(window=20).mean()\n",
    "    rtn_6m = close_rtn.rolling(window=120).mean()\n",
    "    rtn_1y = close_rtn.rolling(window=252).mean()\n",
    "    ##### return ts zscore \n",
    "    rtn_z_1w = zscore(x=close_rtn,window=5)\n",
    "    rtn_z_1m = zscore(x=close_rtn,window=20)\n",
    "    rtn_z_6m = zscore(x=close_rtn,window=120)\n",
    "    rtn_z_1y = zscore(x=close_rtn,window=252)\n",
    "    ##### return ts skew \n",
    "    rtn_skew_1w = close_rtn.rolling(window=5).skew()\n",
    "    rtn_skew_1m = close_rtn.rolling(window=20).skew()\n",
    "    rtn_skew_6m = close_rtn.rolling(window=120).skew()\n",
    "    rtn_skew_1y = close_rtn.rolling(window=252).skew()\n",
    "\n",
    "    ##### return ts diff \n",
    "    # rtn_diff_1w = ts_diff(dt=close_rtn, lookback1=5,lookback2=10)\n",
    "    # rtn_diff_1m = ts_diff(dt=close_rtn, lookback1=20,lookback2=40)\n",
    "    # rtn_diff_6m = ts_diff(dt=close_rtn, lookback1=120,lookback2=240)\n",
    "\n",
    "    ##### up returns \n",
    "    up_rtn = close_rtn.copy()\n",
    "    up_rtn[up_rtn<0] = 0\n",
    "    ##### up return ts mean \n",
    "    up_rtn_1w = up_rtn.rolling(window=5).mean()\n",
    "    up_rtn_1m = up_rtn.rolling(window=20).mean()\n",
    "    up_rtn_6m = up_rtn.rolling(window=120).mean()\n",
    "    up_rtn_1y = up_rtn.rolling(window=252).mean()\n",
    "\n",
    "    ##### down returns \n",
    "    down_rtn = close_rtn.copy()\n",
    "    down_rtn[down_rtn>0] = 0\n",
    "    ##### down return ts mean \n",
    "    down_rtn_1w = down_rtn.rolling(window=5).mean()\n",
    "    down_rtn_1m = down_rtn.rolling(window=20).mean()\n",
    "    down_rtn_6m = down_rtn.rolling(window=120).mean()\n",
    "    down_rtn_1y = down_rtn.rolling(window=252).mean()\n",
    "\n",
    "    ##### adv ts zscore \n",
    "    adv_z_1w = zscore(x=adv_clean,window=5)\n",
    "    adv_z_1m = zscore(x=adv_clean,window=20)\n",
    "    adv_z_6m = zscore(x=adv_clean,window=120)\n",
    "    adv_z_1y = zscore(x=adv_clean,window=252)\n",
    "    ##### adv ts mean \n",
    "    adv_1w = adv_clean.rolling(window=5).mean()\n",
    "    adv_1m = adv_clean.rolling(window=20).mean()\n",
    "    adv_6m = adv_clean.rolling(window=120).mean()\n",
    "    adv_1y = adv_clean.rolling(window=252).mean()\n",
    "\n",
    "    ##### cap ts zscore \n",
    "    cap_z_1w = zscore(x=cap_clean,window=5)\n",
    "    cap_z_1m = zscore(x=cap_clean,window=20)\n",
    "    cap_z_6m = zscore(x=cap_clean,window=120)\n",
    "    cap_z_1y = zscore(x=cap_clean,window=252)\n",
    "    ##### adv ts mean \n",
    "    cap_1w = cap_clean.rolling(window=5).mean()\n",
    "    cap_1m = cap_clean.rolling(window=20).mean()\n",
    "    cap_6m = cap_clean.rolling(window=120).mean()\n",
    "    cap_1y = cap_clean.rolling(window=252).mean()\n",
    "\n",
    "    signals = pd.DataFrame({'fwdrtn':fwd_rtn_20.melt()['value'].values,\n",
    "                            'net_income_growth':features_clean.net_income_growth.values,\n",
    "                            'profit_margin':features_clean.profit_margin.values,\n",
    "                            'pe_ratio':features_clean.pe_ratio.values,\n",
    "                            'ps_ratio':features_clean.ps_ratio.values,\n",
    "                            'technical_indicator':features_clean.technical_indicator.values,\n",
    "                            'rtn_1w':rtn_1w.melt()['value'].values,'rtn_1m':rtn_1m.melt()['value'].values,'rtn_6m':rtn_6m.melt()['value'].values,'rtn_1y':rtn_1y.melt()['value'].values,\n",
    "                            'rtn_z_1w':rtn_z_1w.melt()['value'].values,'rtn_z_1m':rtn_z_1m.melt()['value'].values,'rtn_z_6m':rtn_z_6m.melt()['value'].values,'rtn_z_1y':rtn_z_1y.melt()['value'].values,\n",
    "                           'rtn_skew_1w':rtn_skew_1w.melt()['value'].values,'rtn_skew_1m':rtn_skew_1m.melt()['value'].values,'rtn_skew_6m':rtn_skew_6m.melt()['value'].values,'rtn_skew_1y':rtn_skew_1y.melt()['value'].values,\n",
    "                           'up_rtn_1w':up_rtn_1w.melt()['value'].values,'up_rtn_1m':up_rtn_1m.melt()['value'].values,'up_rtn_6m':up_rtn_6m.melt()['value'].values,'up_rtn_1y':up_rtn_1y.melt()['value'].values,\n",
    "                           'down_rtn_1w':down_rtn_1w.melt()['value'].values,'down_rtn_1m':down_rtn_1m.melt()['value'].values,'down_rtn_6m':down_rtn_6m.melt()['value'].values,'down_rtn_1y':down_rtn_1y.melt()['value'].values,\n",
    "                            'adv_z_1w':adv_z_1w.melt()['value'].values,'adv_z_1m':adv_z_1m.melt()['value'].values,'adv_z_6m':adv_z_6m.melt()['value'].values,'adv_z_1y':adv_z_1y.melt()['value'].values,\n",
    "                            'adv_1w':adv_1w.melt()['value'].values,'adv_1m':adv_1m.melt()['value'].values,'adv_6m':adv_6m.melt()['value'].values,'adv_1y':adv_1y.melt()['value'].values,\n",
    "                            'cap_z_1w':cap_z_1w.melt()['value'].values,'cap_z_1m':cap_z_1m.melt()['value'].values,'cap_z_6m':cap_z_6m.melt()['value'].values,'cap_z_1y':cap_z_1y.melt()['value'].values,\n",
    "\n",
    "                           })\n",
    "    signals.index=features_clean.index\n",
    "    return signals\n",
    "\n",
    "\n",
    "def clean_backtestinput_format(df,start_date,end_date,isfwd=False):\n",
    "    if isfwd:\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df[df.abs() > 1] = np.nan\n",
    "    df = df.sort_index().loc[start_date:end_date]\n",
    "    return df\n",
    "\n",
    "def get_alpha(df,signals,pos_thres=0.0005, neg_thres=-0.0005, col='fwdrtn',isfilter=False):\n",
    "    import warnings\n",
    "    from scipy.linalg import LinAlgWarning\n",
    "    warnings.filterwarnings(action='ignore', category=LinAlgWarning, module='sklearn')\n",
    "    \n",
    "    df = df.copy()\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    correlation = df.corr()\n",
    "    if isfilter:\n",
    "        features = correlation.loc[(correlation.fwdrtn > pos_thres) | (correlation.fwdrtn<neg_thres)][[col]].sort_values(col).index.to_list()\n",
    "    else:\n",
    "        features = correlation.index.to_list()\n",
    "    features.remove(col)\n",
    "    \n",
    "    train = df.dropna(subset=[col]+features)\n",
    "    X = train[features]\n",
    "    Y = train[col]\n",
    "    model = Ridge(alpha=0.51)\n",
    "    Ridgemodel = model.fit(X,Y)\n",
    "    \n",
    "    available = df.dropna(subset=features).copy()\n",
    "    \n",
    "    rst = pd.DataFrame(data=Ridgemodel.predict(available[features]),index=available.index,columns=['alpha'])\n",
    "    alpha = pd.DataFrame(index=signals.index,columns=['alpha'])\n",
    "    tmp = alpha.merge(rst,left_index=True,right_index=True,how='left')\n",
    "    alpha = tmp[['alpha_y']].reset_index().pivot(index='date',columns='variable',values='alpha_y')\n",
    "    \n",
    "    return alpha \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_model(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df.Y = -df.X3\n",
    "    \n",
    "    df.loc[abs(df.X1) < 0.1,'Y'] = 0.1  \n",
    "    df.loc[(abs(df.X1) >= 0.1) & (df.X1*df.X2 < 0) & (df.X1*df.X3 > 0), 'Y'] = df.loc[(abs(df.X1) >= 0.1) & (df.X1*df.X2 < 0) & (df.X1*df.X3 > 0), 'X1'] + df.loc[(abs(df.X1) >= 0.1) & (df.X1*df.X2 < 0) & (df.X1*df.X3 > 0), 'X2'] + df.loc[(abs(df.X1) >= 0.1) & (df.X1*df.X2 < 0) & (df.X1*df.X3 > 0), 'X3']\n",
    "    \n",
    "    \n",
    "    return df \n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backtesting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Correlation(s1, s2, method='pearson'):\n",
    "    corr = None\n",
    "    not_nan_loc = (~np.isnan(s1)) & (~np.isnan(s2))\n",
    "\n",
    "\n",
    "    if not_nan_loc.sum() < 2:\n",
    "        return np.nan\n",
    "\n",
    "    s1 = s1[not_nan_loc]\n",
    "    s2 = s2[not_nan_loc]\n",
    "    \n",
    "    if method == 'pearson':\n",
    "        corr = stats.pearsonr(s1, s2)[0]\n",
    "    elif method == 'spearman':\n",
    "        corr = stats.spearmanr(s1, s2)[0]\n",
    "    return corr\n",
    "\n",
    "def Regression(x, y):\n",
    "    if x.shape[0] < 3:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    \n",
    "    x = x.reshape(-1, 1)\n",
    "    n, k = x.shape[0], 1\n",
    "    reg = LinearRegression().fit(x, y)\n",
    "    y_hat = reg.predict(x)\n",
    "    residual = y - y_hat\n",
    "    coef = reg.coef_\n",
    "    intercept = reg.intercept_\n",
    "\n",
    "    sigma_hat = sum(residual ** 2) / (n - k - 1)  # estimate of error term variance\n",
    "    variance_beta_hat = sigma_hat * np.linalg.inv(np.matmul(x.transpose(), x))\n",
    "    t_stat = coef / np.sqrt(variance_beta_hat.diagonal())\n",
    "    return t_stat, coef, intercept\n",
    "\n",
    "\n",
    "class SingleFactorAnalysis:\n",
    "\n",
    "    def __init__(self, forward_ret, alpha_df, tradable_df, freq):\n",
    "\n",
    "        self.alpha_df = alpha_df.copy()\n",
    "        self.alpha_np = self.alpha_df.values\n",
    "\n",
    "        self.tradable_df = tradable_df\n",
    "        self.tradable_np = self.tradable_df.values\n",
    "\n",
    "        self.alpha_df_tradable = alpha_df.copy()\n",
    "        self.alpha_df_tradable[self.tradable_df == 0] = 0\n",
    "        self.alpha_np_tradable = self.alpha_df_tradable.values\n",
    "\n",
    "        self.fwd_rtn = forward_ret\n",
    "        self.fwd_rtn_np = self.fwd_rtn.values\n",
    "\n",
    "        self.fwd_rtn_norm = self.fwd_rtn.subtract(self.fwd_rtn.mean(axis=1), axis=0)\n",
    "        self.fwd_rtn_norm_np = self.fwd_rtn_norm.values\n",
    "\n",
    "        self.dates_len = self.alpha_np.shape[0]\n",
    "        self.dates_index = self.alpha_df.index\n",
    "        self.sname = self.alpha_df.columns\n",
    "\n",
    "        self.points_per_year = freq\n",
    "\n",
    "\n",
    "        # group\n",
    "        self.group_num = 5\n",
    "        self.group_ptf_rtn_np = np.zeros((self.dates_len, self.group_num))\n",
    "        self.group_ptf_rtn_df = None\n",
    "\n",
    "        # IC\n",
    "        self.IC_np = np.zeros(self.dates_len)\n",
    "        self.IC_series = None\n",
    "\n",
    "        # regression\n",
    "        self.tstats_np = np.zeros(self.dates_len)\n",
    "        self.tstats_series = None\n",
    "\n",
    "        self.factor_ret_np = np.zeros(self.dates_len)\n",
    "        self.factor_ret_series = None\n",
    "\n",
    "        self.factor_alpha_np = np.zeros(self.dates_len)\n",
    "        self.factor_alpha_series = None\n",
    "\n",
    "        self.performance = {}\n",
    "\n",
    "    def Statistics(self):\n",
    "\n",
    "        for i in range(self.dates_len-1):\n",
    "\n",
    "            tradable_loc = self.tradable_np[i, :] == 1\n",
    "\n",
    "            # alpha calculation\n",
    "            alpha_currentdate = self.alpha_np[i, tradable_loc]\n",
    "\n",
    "            fwd_rtn_currentdate = self.fwd_rtn_np[i, tradable_loc]\n",
    "            fwd_rtn_norm_currentdate = self.fwd_rtn_norm_np[i, tradable_loc]\n",
    "\n",
    "            # group portfolios\n",
    "            tradable_num = len(alpha_currentdate)\n",
    "            num_per_group = int(tradable_num/self.group_num)\n",
    "            ind = np.argsort(alpha_currentdate)  # ascending\n",
    "            for j in range(self.group_num):\n",
    "                ind_this_group = ind[j*num_per_group:(j+1)*num_per_group]\n",
    "                fwd_rtn_group = fwd_rtn_currentdate[ind_this_group]\n",
    "                self.group_ptf_rtn_np[i, j] = fwd_rtn_group[~np.isnan(fwd_rtn_group)].mean()\n",
    "\n",
    "            not_nan_loc = (~np.isnan(alpha_currentdate)) & (~np.isnan(fwd_rtn_currentdate))\n",
    "\n",
    "\n",
    "            if (~not_nan_loc).all():\n",
    "                # all alpha/fwd_rtn are nan, skip today\n",
    "                continue\n",
    "\n",
    "            # IC method\n",
    "            self.IC_np[i] = Correlation(alpha_currentdate[not_nan_loc], fwd_rtn_currentdate[not_nan_loc], method='spearman')\n",
    "\n",
    "            # t stats\n",
    "            tstats, factor_return, factor_alpha = Regression(alpha_currentdate[not_nan_loc], fwd_rtn_currentdate[not_nan_loc])\n",
    "            self.tstats_np[i] = tstats\n",
    "            self.factor_ret_np[i] = factor_return\n",
    "            self.factor_alpha_np[i] = factor_alpha\n",
    "\n",
    "        # factor weights\n",
    "\n",
    "        weight_df = self.alpha_df_tradable.div(self.alpha_df_tradable.abs().sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "        # factor portfolio returns\n",
    "        alpha_returns_df = weight_df * self.fwd_rtn\n",
    "        self.ptf_returns = alpha_returns_df.sum(axis=1)\n",
    "\n",
    "        self.group_ptf_rtn_df = pd.DataFrame(self.group_ptf_rtn_np,\n",
    "                                             index=self.dates_index,\n",
    "                                             columns=['group ' + str(i) for i in range(self.group_num, 0, -1)])\n",
    "        self.IC_series = pd.Series(self.IC_np, index=self.dates_index).fillna(0)\n",
    "        self.IC_cum_series = self.IC_series.cumsum()\n",
    "        self.tstats_series = pd.Series(self.tstats_np, index=self.dates_index).fillna(0)\n",
    "        self.factor_ret_series = pd.Series(self.factor_ret_np, index=self.dates_index).fillna(0)\n",
    "        self.factor_alpha_series = pd.Series(self.factor_alpha_np, index=self.dates_index).fillna(0)\n",
    "\n",
    "        self.performance['IC mean'] = self.IC_series.mean()\n",
    "        self.performance['IC std'] = self.IC_series.std()\n",
    "        self.performance['ICIR'] = self.IC_series.mean()/self.IC_series.std() * np.sqrt(self.points_per_year)\n",
    "        self.performance['t-stats mean'] = self.tstats_series.mean()\n",
    "#         self.performance['Annual Return'] = self.ptf_returns.mean() * self.points_per_year\n",
    "        self.performance['Factor Portfolio Return'] = self.ptf_returns.mean() * self.points_per_year\n",
    "        self.performance['Factor Portfolio Sharpe Ratio'] = self.ptf_returns.mean() / self.ptf_returns.std() * np.sqrt(self.points_per_year)\n",
    "\n",
    "    def PlotResult(self):\n",
    "        self._CumulativeAlphaReturns()\n",
    "        self._GroupPortfolios()\n",
    "        self._IC()\n",
    "        self._AlphaDecay()\n",
    "        plt.show()\n",
    "\n",
    "        print(self.performance)\n",
    "\n",
    "\n",
    "    def _CumulativeAlphaReturns(self):\n",
    "\n",
    "        self.performance['AnnualReturn'] = self.ptf_returns.mean() * self.points_per_year\n",
    "\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.plot(self.ptf_returns.cumsum())\n",
    "        plt.title('Factor Weighted Long/Short Portfolio Cumulative Return')\n",
    "\n",
    "    def _GroupPortfolios(self):\n",
    "\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.plot(self.group_ptf_rtn_df.cumsum())\n",
    "        plt.legend(self.group_ptf_rtn_df.columns)\n",
    "        plt.title('Cumulative Return by Quantile')\n",
    "\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.plot((self.group_ptf_rtn_df.iloc[:, -1] - self.group_ptf_rtn_df.iloc[:, 0]).cumsum())\n",
    "        plt.title('Top Minus Bottom Quantile Cumulative Return')\n",
    "\n",
    "        ptf_rtn = pd.DataFrame()\n",
    "        ptf_rtn['Group bottom quantile'] = self.group_ptf_rtn_df['group 5']\n",
    "        ptf_rtn['Group top quantile'] = self.group_ptf_rtn_df['group 1']\n",
    "\n",
    "        # plt.figure(figsize=(16, 9))\n",
    "        # plt.plot(ptf_rtn.cumsum())\n",
    "        # plt.legend(ptf_rtn.columns)\n",
    "        # plt.title('')\n",
    "\n",
    "    def _IC(self):\n",
    "\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.bar(x=self.IC_series.index, height=self.IC_series.values)\n",
    "        plt.plot(self.IC_series.rolling(window=5).mean())\n",
    "        plt.title('IC mean')\n",
    "\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.plot(self.IC_cum_series.values)\n",
    "        plt.title('IC Cumulative Sum')\n",
    "\n",
    "    def _AlphaDecay(self):\n",
    "        alpha_decay_np = np.zeros((self.dates_len, 10))\n",
    "        decay_period = range(alpha_decay_np.shape[1])\n",
    "\n",
    "        for i in range(self.dates_len-decay_period[-1]):\n",
    "            for j in decay_period:\n",
    "                alpha_decay_np[i, j] = Correlation(self.alpha_np[i, :], self.alpha_np[i+j, :])\n",
    "\n",
    "        alpha_decay_df = pd.DataFrame(alpha_decay_np,\n",
    "                                      index=self.dates_index,\n",
    "                                      columns=decay_period).mean(axis=0)\n",
    "\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.plot(alpha_decay_df)\n",
    "        plt.title('Alpha Decay')\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
