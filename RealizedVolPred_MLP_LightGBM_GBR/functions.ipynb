{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "023df295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import arange\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns',999)\n",
    "pd.set_option('display.max_rows',999)\n",
    "pd.set_option('float.format','{:f}'.format)\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import skew\n",
    "import math \n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn import metrics\n",
    "# !pip install threadpoolctl==3.1.0\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86402bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d6a8a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "legpos = 'center left'\n",
    "size = 'medium'\n",
    "loc=(1,0.5)\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set()\n",
    "\n",
    "def visualization(df, x, y, figsize=(12,3), hue=None, scatter=False, dist=False, cust_col='Set2', title='' ,xlabel='', ylabel='', rotation_angel=90):\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore', message=\"Ignoring `palette` because no `hue` variable has been assigned.\")\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    if (scatter):\n",
    "        ax = sns.scatterplot(x=x,y=y,data=df,hue=hue,palette=cust_col)\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.xticks(rotation=rotation_angel)\n",
    "        ax.set(xlabel=xlabel, ylabel=ylabel)\n",
    "        if (hue != None):\n",
    "            plt.legend(loc=legpos,bbox_to_anchor=loc,fontsize=size)\n",
    "            \n",
    "#     elif (dist):\n",
    "#         ax = sns.displot(x=x,y=y,data=df,hue=hue,palette=cust_col)\n",
    "\n",
    "#         plt.title(title)\n",
    "#         plt.xticks(rotation=rotation_angel)\n",
    "#         ax.set(xlabel=xlabel, ylabel=ylabel)\n",
    "\n",
    "    elif (hue != None):\n",
    "        ax = sns.lineplot(x=x,y=y,data=df,hue=hue,palette=cust_col)\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.xticks(rotation=rotation_angel)\n",
    "        plt.legend(loc=legpos,bbox_to_anchor=loc,fontsize=size)\n",
    "        ax.set(xlabel=xlabel, ylabel=ylabel)\n",
    "    else:\n",
    "        ax = sns.lineplot(x=x,y=y,data=df,palette=cust_col)\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.xticks(rotation=rotation_angel)\n",
    "        ax.set(xlabel=xlabel, ylabel=ylabel)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def visualization_3d(df, x, y, z,figsize=(8,8), title='' ,xlabel='', ylabel='', zlabel=''):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.scatter3D(df[x], df[y], df[z], cmap='Greens')\n",
    "    plt.title(title)\n",
    "    ax.set(xlabel=xlabel, ylabel=ylabel,zlabel=zlabel)\n",
    "    plt.show()\n",
    "    \n",
    "def display_features(df,feature='cap',scatter=False):\n",
    "    display(feature + ' start date: '+str(df.index.min()))\n",
    "    display(feature + ' end date: '+str(df.index.max()))\n",
    "    tmp = df.notnull().sum(axis=1).to_frame().rename(columns={0:'cnt'})\n",
    "    if not scatter:\n",
    "        visualization(df=None, x=tmp.index,y=tmp.cnt,title='Number of available securities - '+feature, rotation_angel=0)\n",
    "    else:\n",
    "        visualization(df=None, x=tmp.index,y=tmp.cnt,title='Number of available securities - '+feature, scatter=True, rotation_angel=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a0736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mktcap(file_dir='./data/MARKET_CAP.pickle', date_col='date',col_name='ticker_id'):\n",
    "    df = pd.read_pickle(file_dir)\n",
    "    df = pd.DataFrame(df)\n",
    "    df.index= pd.to_datetime(df.index).tz_localize(None)\n",
    "    df.index.name = date_col\n",
    "    df.columns.name = col_name\n",
    "    df.columns = df.columns.astype(str)\n",
    "    return df\n",
    "\n",
    "def read_rtn(file_dir, date_col='date',col_name='ticker_id'):\n",
    "    df = pd.read_pickle(file_dir)\n",
    "    df = pd.DataFrame(df).reset_index(level='start', drop=True)\n",
    "    df.index= pd.to_datetime(df.index).tz_localize(None)\n",
    "    df.index.name = date_col\n",
    "    df.columns.name = col_name\n",
    "    df.columns = df.columns.astype(str)\n",
    "    return df\n",
    "\n",
    "def read_profit(file_dir,col, date_col='date',idx_col='ticker_id',val='profit'):\n",
    "    df = pd.read_pickle(file_dir)\n",
    "    df = pd.DataFrame(df).reset_index()\n",
    "    df.columns = col\n",
    "    \n",
    "    df[date_col] = pd.to_datetime(df[date_col].dt.date)\n",
    "    df[idx_col] = df[idx_col].astype(str)\n",
    "    \n",
    "    df.loc[:,'quarter'] = df[date_col].dt.year.astype(str)+'-Q'+df[date_col].dt.quarter.astype(str)\n",
    "    df.loc[:,'month'] = df[date_col].dt.year.astype(str)+'-'+df[date_col].dt.month.astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_sector(file_dir, col, date_col='date',idx_col='ticker_id'):\n",
    "    \n",
    "    df = pd.read_pickle(file_dir)\n",
    "    df = pd.DataFrame(df).reset_index()\n",
    "    df.columns = col\n",
    "    df[idx_col] = df[idx_col].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_macro(file_dir, col, date_col='date'):\n",
    "    \n",
    "    df = pd.read_pickle(file_dir)\n",
    "    df = pd.DataFrame(df).reset_index()\n",
    "    df.columns = col\n",
    "    df[date_col] = pd.to_datetime(df[date_col].dt.date)\n",
    "\n",
    "    return df\n",
    "\n",
    "def missing_values(df):\n",
    "    tmp = df.isnull().sum().to_frame().T\n",
    "    display(tmp)\n",
    "    missing_ticker = tmp.loc[0][tmp.loc[0] != 0].index.tolist()\n",
    "    return missing_ticker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7111e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cddb3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def universe_selection(df, low_thred=100, high_thred=900):\n",
    "    \n",
    "    rst = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "    tmp = df.copy()\n",
    "\n",
    "    for row in tmp.index:\n",
    "        univ = tmp.loc[row,:].sort_values(ascending=False).iloc[low_thred:high_thred].index.to_list()\n",
    "        rst.loc[row,univ] = 1\n",
    "    return rst\n",
    "\n",
    "def whether_in_universe(df):\n",
    "    isin_univ = df.copy()\n",
    "    isin_univ = isin_univ.fillna(0)\n",
    "    isin_univ[isin_univ != 0] = 1.0\n",
    "    isin_univ = isin_univ.astype('int')\n",
    "    return isin_univ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_macro_features(df, start_date, clusters=range(3,6)):\n",
    "    df = df.copy()\n",
    "    df = df.loc[start_date:].fillna(method='ffill')\n",
    "    \n",
    "    ### insample train \n",
    "    for cluster in clusters:\n",
    "        kmeans = KMeans(n_clusters=cluster, random_state=0, n_init=\"auto\").fit(df)\n",
    "        df.loc[:,'macro_group'+str(cluster)] = kmeans.labels_  #kmeans.predict(macro)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def cluster_viz(df, n_cluster=3, col1='NONFARMPAYROLL',col2='UnemploymentRate',n1=3,n2=4):\n",
    "    df = df.copy()\n",
    "    model = KMeans(n_clusters=n_cluster, random_state=0, n_init=\"auto\")\n",
    "    model.fit(df)\n",
    "    labels = model.predict(df)\n",
    "\n",
    "    xs = df.loc[:,col1]\n",
    "    ys = df.loc[:,col2]\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(xs,ys,c=labels,alpha=0.5, cmap='jet')\n",
    "\n",
    "    centroids = model.cluster_centers_\n",
    "    centroids_x = centroids[:,n1]\n",
    "    centroids_y = centroids[:,n2]\n",
    "    \n",
    "    plt.scatter(centroids_x,centroids_y,marker='D',s=50)\n",
    "    plt.title(f'{n_cluster} clusters distribution')\n",
    "    plt.xlabel(col1)\n",
    "    plt.ylabel(col2)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def elblow_check(df):\n",
    "    ks = range(1, 6)\n",
    "    inertias = []\n",
    "    for k in ks:\n",
    "        model = KMeans(n_clusters=k, random_state=0, n_init=\"auto\")\n",
    "        model.fit(df)\n",
    "        inertias.append(model.inertia_)\n",
    "\n",
    "    # Plot ks vs inertias\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(ks, inertias, '-o')\n",
    "    plt.title('Elbow method looks for the best number of clusters')\n",
    "    plt.xlabel('number of clusters, k')\n",
    "    plt.ylabel('inertia')\n",
    "    plt.xticks(ks)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def merge_macro_features(rtn, macro_df,date_col = 'date',idx_col = 'ticker_id'):\n",
    "    rtn = rtn.copy()\n",
    "    fwd_rtn = rtn.shift(-1)\n",
    "    fwd_rtn = fwd_rtn.reset_index().melt(id_vars=date_col).rename(columns={'value':'fwd_rtn'})\n",
    "    fwd_rtn.loc[:,'month'] = fwd_rtn[date_col].dt.year.astype(str)+'-'+fwd_rtn[date_col].dt.month.astype(str)\n",
    " \n",
    "    ##### merge monthly macro features \n",
    "    macro_tmp = macro_df.reset_index()\n",
    "    macro_tmp.loc[:,'month'] = macro_tmp[date_col].dt.year.astype(str)+'-'+macro_tmp[date_col].dt.month.astype(str)\n",
    "    \n",
    "    del macro_tmp['date']\n",
    "    rst = pd.merge(fwd_rtn, macro_tmp, on=['month'], how='left')\n",
    "    del rst['month']\n",
    "    \n",
    "    rst = rst.set_index([date_col,idx_col])\n",
    "    \n",
    "    return rst\n",
    "\n",
    "def get_profit_features(df, col='month',date_col='date',idx_col='ticker_id',val='profit'):\n",
    "    df = df.copy()\n",
    "    df.loc[:,date_col] = pd.to_datetime(df[col])\n",
    "\n",
    "    df = df.sort_values([date_col, idx_col])\n",
    "\n",
    "    ##### cross-sectional: rank all stocks \n",
    "    df['rank'] = df.groupby(date_col)[val].rank(method='dense', ascending=False).astype(int)\n",
    "\n",
    "    ##### ts trend - quarterly change \n",
    "    def profit_trend(group,n=3):\n",
    "        # Shift the score by 3 months as a quarter\n",
    "        group['profit_trend'] = (group['profit'] > group['profit'].shift(n)).astype(int)\n",
    "        return group\n",
    "    \n",
    "    df = df.groupby(idx_col).apply(profit_trend).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def merge_monthly_features(index_df, macro_df, profit_df,date_col = 'date',idx_col = 'ticker_id'):\n",
    "    multi_index = index_df[[]].reset_index()\n",
    "    multi_index.loc[:,'month'] = multi_index[date_col].dt.year.astype(str)+'-'+multi_index[date_col].dt.month.astype(str)\n",
    "\n",
    "    ##### merge monthly profit features\n",
    "    rst = pd.merge(multi_index, profit_df[['month','ticker_id','profit','rank','profit_trend']], on=['month','ticker_id'], how='left')\n",
    "    \n",
    "    ##### merge monthly macro features \n",
    "    macro_tmp = macro_df.reset_index()\n",
    "    macro_tmp.loc[:,'month'] = macro_tmp[date_col].dt.year.astype(str)+'-'+macro_tmp[date_col].dt.month.astype(str)\n",
    "    del macro_tmp['date']\n",
    "    rst = pd.merge(rst, macro_tmp, on=['month'], how='left')\n",
    "    del rst['month']\n",
    "    \n",
    "    rst = rst.set_index([date_col,idx_col])\n",
    "    \n",
    "    return rst\n",
    "\n",
    "def get_daily_rawinput(date_col = 'date',idx_col = 'ticker_id'):\n",
    "    daily_rtn = rtn.reset_index().melt(id_vars=date_col).set_index([date_col,idx_col]).rename(columns={'value':'daily_rtn'})\n",
    "    fwd_rtn = rtn.shift(-1).reset_index().melt(id_vars=date_col).set_index([date_col,idx_col]).rename(columns={'value':'fwd_rtn'})\n",
    "    mktcap_log = np.log(mktcap).reset_index().melt(id_vars=date_col).set_index([date_col,idx_col]).rename(columns={'value':'mktcap_log'})\n",
    "    daily_mktcap = mktcap.reset_index().melt(id_vars=date_col).set_index([date_col,idx_col]).rename(columns={'value':'mktcap'})\n",
    "\n",
    "    features = fwd_rtn.merge(daily_rtn,left_index=True, right_index=True, how='left')\n",
    "    features = features.merge(mktcap_log,left_index=True, right_index=True, how='left')\n",
    "    features = features.merge(daily_mktcap,left_index=True, right_index=True, how='left')\n",
    "    features = pd.merge(features.reset_index(), sector, on=idx_col, how='left').set_index([date_col,idx_col])\n",
    "\n",
    "    monthly_features = merge_monthly_features(index_df=features, macro_df=macro_features, profit_df=profit_features)\n",
    "    features = features.merge(monthly_features,left_index=True, right_index=True, how='left')\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def fill_missing_values(df,col_list):\n",
    "    ##### get sector avg\n",
    "    sector_avg = df.reset_index().groupby(['date','sector'])[col_list].mean()\n",
    "    \n",
    "    ### rename column \n",
    "    for col in col_list:\n",
    "        sector_avg = sector_avg.rename(columns={col:col+'_sector'})\n",
    "        \n",
    "    ##### merge industry avg \n",
    "    df = df.reset_index().merge(sector_avg.reset_index(), how='left',on=['date','sector'])\n",
    "    \n",
    "    ##### fill missing values \n",
    "    for col in col_list:\n",
    "        df.loc[df[col].isnull(),col]=df.loc[df[col].isnull(),col+'_sector']\n",
    "    \n",
    "    df = df[['date','ticker_id','sector']+col_list].set_index(['date','ticker_id']).sort_index()\n",
    "    return df\n",
    "\n",
    "def plot_correlation_chart(df, target_factor, angel=25, plot=True):\n",
    "    \n",
    "    correlation_matrix = df.corr()\n",
    "    target_correlation = correlation_matrix[target_factor].drop(target_factor, errors='ignore')\n",
    "    if plot:\n",
    "    # Plotting\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        target_correlation.plot(kind='bar')\n",
    "        plt.title(f'Correlation with {target_factor}')\n",
    "        plt.ylabel('Correlation Coefficient')\n",
    "        plt.xlabel('Factors')\n",
    "        plt.xticks(rotation=angel)\n",
    "        plt.show()\n",
    "    return target_correlation.to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5e9387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(x, window):\n",
    "    r = x.rolling(window=window, min_periods=5)\n",
    "    m = r.mean()\n",
    "    s = r.std()\n",
    "    z = (x-m)/s\n",
    "    return z\n",
    "\n",
    "def calculate_close_price(df):\n",
    "    ##### relative close price, Pn/P0\n",
    "    rst = np.cumprod(1+df)\n",
    "    return rst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d63c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signals(rtn, features_all,date_col = 'date',idx_col = 'ticker_id'):\n",
    "    \n",
    "    ######## return \n",
    "    close_rtn = rtn.copy()\n",
    "    fwd_rtn = rtn.shift(-1)\n",
    "    \n",
    "    ##### relative price \n",
    "    close_price = calculate_close_price(df=rtn)\n",
    "    \n",
    "    ##### return ts mean \n",
    "    rtn_1w = close_rtn.rolling(window=5).mean()\n",
    "    rtn_1m = close_rtn.rolling(window=20).mean()\n",
    "    rtn_6m = close_rtn.rolling(window=120).mean()\n",
    "    rtn_1y = close_rtn.rolling(window=252).mean()\n",
    "    \n",
    "    ##### return ts zscore \n",
    "    rtn_z_1w = zscore(x=close_rtn,window=5)\n",
    "    rtn_z_1m = zscore(x=close_rtn,window=20)\n",
    "    rtn_z_6m = zscore(x=close_rtn,window=120)\n",
    "    rtn_z_1y = zscore(x=close_rtn,window=252)\n",
    "    \n",
    "    ##### return ts skew \n",
    "    rtn_skew_1w = close_rtn.rolling(window=5).skew()\n",
    "    rtn_skew_1m = close_rtn.rolling(window=20).skew()\n",
    "    rtn_skew_6m = close_rtn.rolling(window=120).skew()\n",
    "    rtn_skew_1y = close_rtn.rolling(window=252).skew()\n",
    "\n",
    "    ##### return ts diff \n",
    "    # rtn_diff_1w = ts_diff(dt=close_rtn, lookback1=5,lookback2=10)\n",
    "    # rtn_diff_1m = ts_diff(dt=close_rtn, lookback1=20,lookback2=40)\n",
    "    # rtn_diff_6m = ts_diff(dt=close_rtn, lookback1=120,lookback2=240)\n",
    "\n",
    "    ##### up returns \n",
    "    up_rtn = close_rtn.copy()\n",
    "    up_rtn[up_rtn<0] = 0\n",
    "    ##### up return ts mean \n",
    "    up_rtn_1w = up_rtn.rolling(window=5).mean()\n",
    "    up_rtn_1m = up_rtn.rolling(window=20).mean()\n",
    "    up_rtn_6m = up_rtn.rolling(window=120).mean()\n",
    "    up_rtn_1y = up_rtn.rolling(window=252).mean()\n",
    "\n",
    "    ##### down returns \n",
    "    down_rtn = close_rtn.copy()\n",
    "    down_rtn[down_rtn>0] = 0\n",
    "    ##### down return ts mean \n",
    "    down_rtn_1w = down_rtn.rolling(window=5).mean()\n",
    "    down_rtn_1m = down_rtn.rolling(window=20).mean()\n",
    "    down_rtn_6m = down_rtn.rolling(window=120).mean()\n",
    "    down_rtn_1y = down_rtn.rolling(window=252).mean()\n",
    "\n",
    "    cap = features_all[['mktcap_log']].reset_index().pivot(index='date',columns=idx_col,values='mktcap_log')\n",
    "    ##### mktcap ts zscore \n",
    "    cap_z_1w = zscore(x=cap,window=5)\n",
    "    cap_z_1m = zscore(x=cap,window=20)\n",
    "    cap_z_6m = zscore(x=cap,window=120)\n",
    "    cap_z_1y = zscore(x=cap,window=252)\n",
    "    ##### mktcap ts mean \n",
    "    cap_1w = cap.rolling(window=5, min_periods=5).mean()\n",
    "    cap_1m = cap.rolling(window=20, min_periods=5).mean()\n",
    "    cap_6m = cap.rolling(window=120, min_periods=5).mean()\n",
    "    cap_1y = cap.rolling(window=252, min_periods=5).mean()\n",
    "    \n",
    "    signals = pd.DataFrame({\n",
    "#         {'fwdrtn':fwd_rtn.melt()['value'].values,\n",
    "#          'close_rtn':close_rtn.melt()['value'].values,\n",
    "#          'profit_rank':features_all['rank'].values,\n",
    "#          'profit_trend':features_all['profit_trend'].values,\n",
    "#          'macro_group':features_all['macro_group'].values,\n",
    "                        \n",
    "        'rtn_1w':rtn_1w.melt()['value'].values,'rtn_1m':rtn_1m.melt()['value'].values,'rtn_6m':rtn_6m.melt()['value'].values,'rtn_1y':rtn_1y.melt()['value'].values,\n",
    "        'rtn_z_1w':rtn_z_1w.melt()['value'].values,'rtn_z_1m':rtn_z_1m.melt()['value'].values,'rtn_z_6m':rtn_z_6m.melt()['value'].values,'rtn_z_1y':rtn_z_1y.melt()['value'].values,\n",
    "       'rtn_skew_1w':rtn_skew_1w.melt()['value'].values,'rtn_skew_1m':rtn_skew_1m.melt()['value'].values,'rtn_skew_6m':rtn_skew_6m.melt()['value'].values,'rtn_skew_1y':rtn_skew_1y.melt()['value'].values,\n",
    "       'up_rtn_1w':up_rtn_1w.melt()['value'].values,'up_rtn_1m':up_rtn_1m.melt()['value'].values,'up_rtn_6m':up_rtn_6m.melt()['value'].values,'up_rtn_1y':up_rtn_1y.melt()['value'].values,\n",
    "       'down_rtn_1w':down_rtn_1w.melt()['value'].values,'down_rtn_1m':down_rtn_1m.melt()['value'].values,'down_rtn_6m':down_rtn_6m.melt()['value'].values,'down_rtn_1y':down_rtn_1y.melt()['value'].values,\n",
    "        'cap_1w':cap_1w.melt()['value'].values,'cap_1m':cap_1m.melt()['value'].values,'cap_6m':cap_6m.melt()['value'].values,'cap_1y':cap_1y.melt()['value'].values,\n",
    "         'cap_z_1w':cap_z_1w.melt()['value'].values,'cap_z_1m':cap_z_1m.melt()['value'].values,'cap_z_6m':cap_z_6m.melt()['value'].values,'cap_z_1y':cap_z_1y.melt()['value'].values,\n",
    "                       })\n",
    "\n",
    "#     signals = pd.DataFrame({'fwdrtn':fwd_rtn.melt()['value'].values,\n",
    "#                             'rtn_1w':rtn_1w.melt()['value'].values,'rtn_1m':rtn_1m.melt()['value'].values,'rtn_6m':rtn_6m.melt()['value'].values,'rtn_1y':rtn_1y.melt()['value'].values,\n",
    "#                             'rtn_z_1w':rtn_z_1w.melt()['value'].values,'rtn_z_1m':rtn_z_1m.melt()['value'].values,'rtn_z_6m':rtn_z_6m.melt()['value'].values,'rtn_z_1y':rtn_z_1y.melt()['value'].values,\n",
    "#                            'rtn_skew_1w':rtn_skew_1w.melt()['value'].values,'rtn_skew_1m':rtn_skew_1m.melt()['value'].values,'rtn_skew_6m':rtn_skew_6m.melt()['value'].values,'rtn_skew_1y':rtn_skew_1y.melt()['value'].values,\n",
    "#                            'up_rtn_1w':up_rtn_1w.melt()['value'].values,'up_rtn_1m':up_rtn_1m.melt()['value'].values,'up_rtn_6m':up_rtn_6m.melt()['value'].values,'up_rtn_1y':up_rtn_1y.melt()['value'].values,\n",
    "#                            'down_rtn_1w':down_rtn_1w.melt()['value'].values,'down_rtn_1m':down_rtn_1m.melt()['value'].values,'down_rtn_6m':down_rtn_6m.melt()['value'].values,'down_rtn_1y':down_rtn_1y.melt()['value'].values\n",
    "#                            })\n",
    "\n",
    "#     signals.index=rtn.index\n",
    "    tmp = rtn.reset_index().melt(id_vars=date_col).set_index([date_col,idx_col])\n",
    "    signals.index=tmp.index\n",
    "    \n",
    "    return signals\n",
    "\n",
    "def get_time_signals(df):\n",
    "    df = df.copy()\n",
    "    weekday_map = {'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5}\n",
    "    \n",
    "    df.loc[:,'Week_Number'] = df.index.get_level_values('date').day_name().map(weekday_map)\n",
    "    df.loc[:,'Month'] = df.index.get_level_values('date').month\n",
    "    df.loc[:,'Quarter'] = df.index.get_level_values('date').quarter\n",
    "    \n",
    "    return df \n",
    "    \n",
    "    \n",
    "def PCA_factors(df):\n",
    "    ##### data cleaning \n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df = df.copy()\n",
    "    df_imputed = df.copy()\n",
    "    df_imputed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_imputed = imputer.fit_transform(df_imputed)\n",
    "    \n",
    "    ##### PCA factors \n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(df_imputed)\n",
    "    n_components = len(pca.explained_variance_ratio_)\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cum_explained_variance = np.cumsum(explained_variance)\n",
    "    idx = np.arange(n_components)+1\n",
    "    df_explained_variance = pd.DataFrame([explained_variance, cum_explained_variance], \n",
    "                                         index=['explained variance', 'cumulative'], \n",
    "                                         columns=idx).T\n",
    "    mean_explained_variance = df_explained_variance.iloc[:,0].mean() # \n",
    "    print('PCA Overview')\n",
    "    print('='*40)\n",
    "    print(\"Total: {} components\".format(n_components))\n",
    "    print('-'*40)\n",
    "    print('Mean explained variance:', round(mean_explained_variance,3))\n",
    "    print('-'*40)\n",
    "    print(df_explained_variance.head(20))\n",
    "    print('-'*40)\n",
    "    limit = int(input(\"Limit scree plot to nth component (0 for all) > \"))\n",
    "    if limit > 0:\n",
    "        limit_df = limit\n",
    "    else:\n",
    "        limit_df = n_components\n",
    "    \n",
    "    ##### explaining power \n",
    "    df_explained_variance_limited = df_explained_variance.iloc[:limit_df,:]\n",
    "    #make scree plot\n",
    "    fig, ax1 = plt.subplots(figsize=(15,6))\n",
    "    ax1.set_title('Explained variance across principal components', fontsize=14)\n",
    "    ax1.set_xlabel('Principal component', fontsize=12)\n",
    "    ax1.set_ylabel('Explained variance', fontsize=12)\n",
    "    ax2 = sns.barplot(x=idx[:limit_df], y='explained variance', data=df_explained_variance_limited, palette='summer')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.grid(False)\n",
    "    ax2.set_ylabel('Cumulative', fontsize=14)\n",
    "    ax2 = sns.lineplot(x=idx[:limit_df]-1, y='cumulative', data=df_explained_variance_limited, color='#fc8d59')\n",
    "    ax1.axhline(mean_explained_variance, ls='--', color='#fc8d59') #plot mean\n",
    "    ax1.text(-.8, mean_explained_variance+(mean_explained_variance*.05), \"average\", color='#fc8d59', fontsize=14) #label y axis\n",
    "    max_y1 = max(df_explained_variance_limited.iloc[:,0])\n",
    "    max_y2 = max(df_explained_variance_limited.iloc[:,1])\n",
    "    ax1.set(ylim=(0, max_y1+max_y1*.1))\n",
    "    ax2.set(ylim=(0, max_y2+max_y2*.1))\n",
    "    plt.show()\n",
    "    \n",
    "    loadings = pca.components_.T[:, :3]  # Only take the first 3 components\n",
    "\n",
    "    #top 3 loadings for better readability\n",
    "#     df_loadings = pd.DataFrame(loadings, index=df.columns, columns=['PC1', 'PC2', 'PC3'])\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     sns.heatmap(df_loadings, annot=True, cmap='coolwarm', center=0)\n",
    "#     plt.title('Heatmap of Top 3 PCA Component Loadings')\n",
    "#     plt.xlabel('Principal Components')\n",
    "#     plt.ylabel('Original Features')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "#     ##### PC factors \n",
    "    pca = PCA(n_components=5)\n",
    "    principalComponents = pca.fit_transform(df_imputed)\n",
    "    principalDf = pd.DataFrame(data=principalComponents, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5'])\n",
    "    for i in range(1, 6):\n",
    "        df[f'PC{i}'] = principalComponents[:, i-1]\n",
    "\n",
    "    return df\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c408b286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha(df,signals,pos_thres=0.002, neg_thres=-0.002, col='fwdrtn',isfilter=True,idx_col='ticker_id'):\n",
    "    import warnings\n",
    "    from scipy.linalg import LinAlgWarning\n",
    "    warnings.filterwarnings(action='ignore', category=LinAlgWarning, module='sklearn')\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    sector_dummy = pd.get_dummies(data=df['sector'],prefix='sector').astype(int)\n",
    "    df = pd.concat([df,sector_dummy],axis=1).drop('sector', axis=1)\n",
    "\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    correlation = df.corr()\n",
    "    if isfilter:\n",
    "        features = correlation.loc[(correlation[col] > pos_thres) | (correlation[col]<neg_thres)][[col]].sort_values(col).index.to_list()\n",
    "    else:\n",
    "        features = correlation.index.to_list()\n",
    "    \n",
    "    features.remove(col)\n",
    "    \n",
    "    train = df.dropna(subset=[col]+features)\n",
    "    X = train[features]\n",
    "    Y = train[col]\n",
    "    model = Ridge(alpha=0.51)\n",
    "    Ridgemodel = model.fit(X,Y)\n",
    "    \n",
    "    coefficients = model.coef_\n",
    "    # Create a DataFrame for easier interpretation\n",
    "    coef_df = pd.DataFrame(coefficients, index=X.columns, columns=['Coefficient'])\n",
    "    # Sort the DataFrame by the absolute values of the coefficients in descending order\n",
    "    coef_df['Abs_Coefficient'] = coef_df['Coefficient'].abs()\n",
    "    sorted_coef_df = coef_df.sort_values(by='Abs_Coefficient', ascending=False).drop('Abs_Coefficient', axis=1)\n",
    "\n",
    "    available = df.dropna(subset=features).copy()\n",
    "    \n",
    "    rst = pd.DataFrame(data=Ridgemodel.predict(available[features]),index=available.index,columns=['alpha'])\n",
    "    alpha = pd.DataFrame(index=signals.index,columns=['alpha'])\n",
    "    tmp = alpha.merge(rst,left_index=True,right_index=True,how='left')\n",
    "    alpha = tmp[['alpha_y']].reset_index().pivot(index='date',columns=idx_col,values='alpha_y')\n",
    "    \n",
    "    return sorted_coef_df, alpha \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb53d5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_betas(df):\n",
    "    df = df.copy()\n",
    "    ### calculate the daily average across all tickers\n",
    "    daily_avg = df.mean(axis=1)\n",
    "\n",
    "    rolling_beta = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "\n",
    "    ### calculate the 1-year rolling beta for each stock\n",
    "    for ticker in df.columns:\n",
    "        tmp = pd.concat([df[ticker], daily_avg], axis=1)\n",
    "        rolling_cov = tmp.rolling(window=252, min_periods=1).cov().iloc[0::2, -1]\n",
    "        rolling_var = daily_avg.rolling(window=252, min_periods=1).var()\n",
    "\n",
    "        rolling_beta.loc[:,ticker] = (rolling_cov/rolling_var).values\n",
    "    return rolling_beta\n",
    "\n",
    "\n",
    "\n",
    "def adjust_market_impact(data,univ,dt,signal,col='momentum'):\n",
    "   \n",
    "    ##### adjust for market impacts - beta \n",
    "    ### get beta \n",
    "    beta = get_betas(univ_brtn=data)\n",
    "    \n",
    "    ##### adjust for sector impacts \n",
    "    ### create dummy variables for sector\n",
    "    sector = univ.loc[(univ.index.get_level_values('date')==dt)][['sector']]\n",
    "    sector = pd.get_dummies(data=sector['variable'],prefix='sector').droplevel('date')\n",
    "    \n",
    "    ##### perform linear regression to adjust market and sector impacts \n",
    "    X = pd.concat([beta,sector], axis=1)\n",
    "    adj_signal = signal - LinearRegression().fit(X, signal).predict(X)\n",
    "    \n",
    "    adj_signal.name = col\n",
    "    # another way to adjust mkt & sector impacts can be: perform stepwise regression\n",
    "    # adjust mkt impacts first \n",
    "    # then adjust for sector impacts \n",
    "    return adj_signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d78763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_backtestinput_format(df,start_date,end_date,isfwd=False):\n",
    "    if isfwd:\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df[df.abs() > 1] = np.nan\n",
    "    df = df.sort_index().loc[start_date:end_date]\n",
    "    return df\n",
    "\n",
    "def Correlation(s1, s2, method='pearson'):\n",
    "    corr = None\n",
    "    not_nan_loc = (~np.isnan(s1)) & (~np.isnan(s2))\n",
    "\n",
    "\n",
    "    if not_nan_loc.sum() < 2:\n",
    "        return np.nan\n",
    "\n",
    "    s1 = s1[not_nan_loc]\n",
    "    s2 = s2[not_nan_loc]\n",
    "    \n",
    "    if method == 'pearson':\n",
    "        corr = stats.pearsonr(s1, s2)[0]\n",
    "    elif method == 'spearman':\n",
    "        corr = stats.spearmanr(s1, s2)[0]\n",
    "    return corr\n",
    "\n",
    "def Regression(x, y):\n",
    "    if x.shape[0] < 3:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    \n",
    "    x = x.reshape(-1, 1)\n",
    "    n, k = x.shape[0], 1\n",
    "    reg = LinearRegression().fit(x, y)\n",
    "    y_hat = reg.predict(x)\n",
    "    residual = y - y_hat\n",
    "    coef = reg.coef_\n",
    "    intercept = reg.intercept_\n",
    "\n",
    "    sigma_hat = sum(residual ** 2) / (n - k - 1)  # estimate of error term variance\n",
    "    variance_beta_hat = sigma_hat * np.linalg.inv(np.matmul(x.transpose(), x))\n",
    "    t_stat = coef / np.sqrt(variance_beta_hat.diagonal())\n",
    "    return t_stat, coef, intercept\n",
    "\n",
    "class SingleFactorAnalysis:\n",
    "\n",
    "    def __init__(self, forward_ret, alpha_df, tradable_df, freq):\n",
    "\n",
    "        self.alpha_df = alpha_df.copy()\n",
    "        self.alpha_np = self.alpha_df.values\n",
    "\n",
    "        self.tradable_df = tradable_df\n",
    "        self.tradable_np = self.tradable_df.values\n",
    "\n",
    "        self.alpha_df_tradable = alpha_df.copy()\n",
    "        self.alpha_df_tradable[self.tradable_df == 0] = 0\n",
    "        self.alpha_np_tradable = self.alpha_df_tradable.values\n",
    "\n",
    "        self.fwd_rtn = forward_ret\n",
    "        self.fwd_rtn_np = self.fwd_rtn.values\n",
    "\n",
    "        self.fwd_rtn_norm = self.fwd_rtn.subtract(self.fwd_rtn.mean(axis=1), axis=0)\n",
    "        self.fwd_rtn_norm_np = self.fwd_rtn_norm.values\n",
    "\n",
    "        self.dates_len = self.alpha_np.shape[0]\n",
    "        self.dates_index = self.alpha_df.index\n",
    "        self.sname = self.alpha_df.columns\n",
    "\n",
    "        self.points_per_year = freq\n",
    "        self.turnover = 0 \n",
    "\n",
    "        # group\n",
    "        self.group_num = 5\n",
    "        self.group_ptf_rtn_np = np.zeros((self.dates_len, self.group_num))\n",
    "        self.group_ptf_rtn_df = None\n",
    "\n",
    "        # IC\n",
    "        self.IC_np = np.zeros(self.dates_len)\n",
    "        self.IC_series = None\n",
    "\n",
    "        # regression\n",
    "        self.tstats_np = np.zeros(self.dates_len)\n",
    "        self.tstats_series = None\n",
    "\n",
    "        self.factor_ret_np = np.zeros(self.dates_len)\n",
    "        self.factor_ret_series = None\n",
    "\n",
    "        self.factor_alpha_np = np.zeros(self.dates_len)\n",
    "        self.factor_alpha_series = None\n",
    "\n",
    "        self.performance = {}\n",
    "\n",
    "        \n",
    "    def Statistics(self):\n",
    "\n",
    "        for i in range(self.dates_len-1):\n",
    "\n",
    "            tradable_loc = self.tradable_np[i, :] == 1\n",
    "\n",
    "            # alpha calculation\n",
    "            alpha_currentdate = self.alpha_np[i, tradable_loc]\n",
    "\n",
    "            fwd_rtn_currentdate = self.fwd_rtn_np[i, tradable_loc]\n",
    "            fwd_rtn_norm_currentdate = self.fwd_rtn_norm_np[i, tradable_loc]\n",
    "\n",
    "            # group portfolios\n",
    "            tradable_num = len(alpha_currentdate)\n",
    "            num_per_group = int(tradable_num/self.group_num)\n",
    "            ind = np.argsort(alpha_currentdate)  # ascending\n",
    "            \n",
    "            for j in range(self.group_num):\n",
    "                ind_this_group = ind[j*num_per_group:(j+1)*num_per_group]\n",
    "                fwd_rtn_group = fwd_rtn_currentdate[ind_this_group]\n",
    "                self.group_ptf_rtn_np[i, j] = fwd_rtn_group[~np.isnan(fwd_rtn_group)].mean()\n",
    "\n",
    "            not_nan_loc = (~np.isnan(alpha_currentdate)) & (~np.isnan(fwd_rtn_currentdate))\n",
    "\n",
    "            if (~not_nan_loc).all():\n",
    "                # all alpha/fwd_rtn are nan, skip today\n",
    "                continue\n",
    "\n",
    "            # IC method\n",
    "            self.IC_np[i] = Correlation(alpha_currentdate[not_nan_loc], fwd_rtn_currentdate[not_nan_loc], method='spearman')\n",
    "\n",
    "            # t stats\n",
    "            tstats, factor_return, factor_alpha = Regression(alpha_currentdate[not_nan_loc], fwd_rtn_currentdate[not_nan_loc])\n",
    "            self.tstats_np[i] = tstats\n",
    "            self.factor_ret_np[i] = factor_return\n",
    "            self.factor_alpha_np[i] = factor_alpha\n",
    "\n",
    "        # factor weights - alpha weighted pnl to make sure the daily GMV is constant. \n",
    "        weight_df = self.alpha_df_tradable.div(self.alpha_df_tradable.abs().sum(axis=1), axis=0).fillna(0)\n",
    "        \n",
    "        # turnover \n",
    "        self.turnover = weight_df.diff().abs().sum(axis=1)/weight_df.abs().sum(axis=1)\n",
    "        self.turnover.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        self.turnover = self.turnover.fillna(0)\n",
    "\n",
    "        # factor portfolio returns\n",
    "        alpha_returns_df = weight_df * self.fwd_rtn\n",
    "        self.ptf_returns = alpha_returns_df.sum(axis=1)\n",
    "\n",
    "        self.group_ptf_rtn_df = pd.DataFrame(self.group_ptf_rtn_np,\n",
    "                                             index=self.dates_index,\n",
    "                                             columns=['group ' + str(i) for i in range(self.group_num, 0, -1)])\n",
    "        \n",
    "        self.IC_series = pd.Series(self.IC_np, index=self.dates_index).fillna(0)\n",
    "        self.IC_cum_series = self.IC_series.cumsum()\n",
    "        self.tstats_series = pd.Series(self.tstats_np, index=self.dates_index).fillna(0)\n",
    "        self.factor_ret_series = pd.Series(self.factor_ret_np, index=self.dates_index).fillna(0)\n",
    "        self.factor_alpha_series = pd.Series(self.factor_alpha_np, index=self.dates_index).fillna(0)\n",
    "\n",
    "        self.performance['IC mean'] = self.IC_series.mean()\n",
    "        self.performance['IC std'] = self.IC_series.std()\n",
    "        self.performance['ICIR'] = self.IC_series.mean()/self.IC_series.std() * np.sqrt(self.points_per_year)\n",
    "        self.performance['t-stats mean'] = self.tstats_series.mean()\n",
    "        self.performance['Factor Portfolio Return'] = self.ptf_returns.mean() * self.points_per_year\n",
    "        self.performance['Factor Portfolio Sharpe Ratio'] = self.ptf_returns.mean() / self.ptf_returns.std() * np.sqrt(self.points_per_year)\n",
    "        self.performance['Turnover'] = self.turnover.mean()\n",
    "        \n",
    "    def PlotResult(self):\n",
    "        self._CumulativeAlphaReturns()\n",
    "        self._Drawdown()\n",
    "        self._Turnover()\n",
    "        self._GroupPortfolios()\n",
    "        self._IC()\n",
    "        self._AlphaDecay()\n",
    "        plt.show()\n",
    "        \n",
    "        self.performance = {key: f\"{value:.3f}\" for key, value in self.performance.items()}\n",
    "        print(self.performance)\n",
    "    \n",
    "    def _Drawdown(self):\n",
    "        cumrtn = self.ptf_returns.cumsum()\n",
    "        nav = cumrtn+1\n",
    "        max_nav = nav.apply(lambda x: max(1,x)).cummax()\n",
    "\n",
    "        # Calculate drawdown\n",
    "        drawdown = nav - max_nav\n",
    "\n",
    "        # Calculate drawdown percentage\n",
    "        drawdown_percentage = 100*drawdown / max_nav\n",
    "        \n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.plot(drawdown_percentage)\n",
    "        plt.title('Portfolio Drawdown')\n",
    "        \n",
    "    def _Turnover(self):\n",
    "        \n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.plot(self.turnover)\n",
    "        plt.title('Portfolio Turnover')\n",
    "\n",
    "    def _CumulativeAlphaReturns(self):\n",
    "\n",
    "        self.performance['AnnualReturn'] = self.ptf_returns.mean() * self.points_per_year\n",
    "\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.plot(self.ptf_returns.cumsum())\n",
    "        plt.title('Factor Weighted Long/Short Portfolio Cumulative Return')\n",
    "\n",
    "    def _GroupPortfolios(self):\n",
    "\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.plot(self.group_ptf_rtn_df.cumsum())\n",
    "        plt.legend(self.group_ptf_rtn_df.columns)\n",
    "        plt.title('Cumulative Return by Quantile')\n",
    "\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.plot((self.group_ptf_rtn_df.iloc[:, -1] - self.group_ptf_rtn_df.iloc[:, 0]).cumsum())\n",
    "        plt.title('Top Minus Bottom Quantile Cumulative Return')\n",
    "\n",
    "        ptf_rtn = pd.DataFrame()\n",
    "        ptf_rtn['Group bottom quantile'] = self.group_ptf_rtn_df['group 5']\n",
    "        ptf_rtn['Group top quantile'] = self.group_ptf_rtn_df['group 1']\n",
    "\n",
    "        # plt.figure(figsize=(16, 9))\n",
    "        # plt.plot(ptf_rtn.cumsum())\n",
    "        # plt.legend(ptf_rtn.columns)\n",
    "        # plt.title('')\n",
    "\n",
    "    def _IC(self):\n",
    "\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.bar(x=self.IC_series.index, height=self.IC_series.values)\n",
    "        plt.plot(self.IC_series.rolling(window=5).mean())\n",
    "        plt.title('IC mean')\n",
    "\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.plot(self.IC_cum_series.values)\n",
    "        plt.title('IC Cumulative Sum')\n",
    "\n",
    "    def _AlphaDecay(self):\n",
    "        alpha_decay_np = np.zeros((self.dates_len, 10))\n",
    "        decay_period = range(alpha_decay_np.shape[1])\n",
    "\n",
    "        for i in range(self.dates_len-decay_period[-1]):\n",
    "            for j in decay_period:\n",
    "                alpha_decay_np[i, j] = Correlation(self.alpha_np[i, :], self.alpha_np[i+j, :])\n",
    "\n",
    "        alpha_decay_df = pd.DataFrame(alpha_decay_np,\n",
    "                                      index=self.dates_index,\n",
    "                                      columns=decay_period).mean(axis=0)\n",
    "\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.plot(alpha_decay_df)\n",
    "        plt.title('Alpha Decay')\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eaee09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e223da1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b107189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
